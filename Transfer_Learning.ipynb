{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff2c9b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2022-09-24 09:08:27.691472: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-24 09:08:27.692906: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 22, 22)            176       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 22, 22)            88        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 22)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 22)            3410      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 22)            88        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 22)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 10, 44)            6820      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 10, 44)            176       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 44)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 4, 44)             13596     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 44)             176       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 44)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 176)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 176)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 22)                3894      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 23        \n",
      "=================================================================\n",
      "Total params: 28,447\n",
      "Trainable params: 28,183\n",
      "Non-trainable params: 264\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1_input (InputLayer)  (None, 22, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 22, 22)            176       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 22, 22)            88        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 22)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 22)            3410      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 22)            88        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 22)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 10, 44)            6820      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 10, 44)            176       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 44)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 4, 44)             13596     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 44)             176       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 44)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 176)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 176)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 177       \n",
      "=================================================================\n",
      "Total params: 24,707\n",
      "Trainable params: 3,851\n",
      "Non-trainable params: 20,856\n",
      "_________________________________________________________________\n",
      "None\n",
      "(800, 1011)\n",
      "(160, 1011)\n",
      "Train on 800 samples, validate on 160 samples\n",
      "Epoch 1/500\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.8151 - accuracy: 0.5875 - val_loss: 0.4962 - val_accuracy: 0.8062\n",
      "Epoch 2/500\n",
      "800/800 [==============================] - 0s 595us/step - loss: 0.5023 - accuracy: 0.7513 - val_loss: 0.3575 - val_accuracy: 0.9000\n",
      "Epoch 3/500\n",
      "800/800 [==============================] - 0s 573us/step - loss: 0.3776 - accuracy: 0.8363 - val_loss: 0.2857 - val_accuracy: 0.9125\n",
      "Epoch 4/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.3551 - accuracy: 0.8550 - val_loss: 0.2509 - val_accuracy: 0.9250\n",
      "Epoch 5/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.2969 - accuracy: 0.8913 - val_loss: 0.2128 - val_accuracy: 0.9375\n",
      "Epoch 6/500\n",
      "800/800 [==============================] - 1s 721us/step - loss: 0.2604 - accuracy: 0.8963 - val_loss: 0.1820 - val_accuracy: 0.9438\n",
      "Epoch 7/500\n",
      "800/800 [==============================] - 1s 640us/step - loss: 0.2469 - accuracy: 0.9025 - val_loss: 0.1614 - val_accuracy: 0.9438\n",
      "Epoch 8/500\n",
      "800/800 [==============================] - 1s 625us/step - loss: 0.2316 - accuracy: 0.9112 - val_loss: 0.1395 - val_accuracy: 0.9625\n",
      "Epoch 9/500\n",
      "800/800 [==============================] - 0s 596us/step - loss: 0.2359 - accuracy: 0.9112 - val_loss: 0.1261 - val_accuracy: 0.9750\n",
      "Epoch 10/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.1852 - accuracy: 0.9312 - val_loss: 0.1147 - val_accuracy: 0.9875\n",
      "Epoch 11/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.1691 - accuracy: 0.9362 - val_loss: 0.1032 - val_accuracy: 0.9750\n",
      "Epoch 12/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.1685 - accuracy: 0.9312 - val_loss: 0.0995 - val_accuracy: 0.9812\n",
      "Epoch 13/500\n",
      "800/800 [==============================] - 1s 694us/step - loss: 0.1503 - accuracy: 0.9438 - val_loss: 0.0978 - val_accuracy: 0.9750\n",
      "Epoch 14/500\n",
      "800/800 [==============================] - 1s 634us/step - loss: 0.1294 - accuracy: 0.9488 - val_loss: 0.0930 - val_accuracy: 0.9750\n",
      "Epoch 15/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.1444 - accuracy: 0.9438 - val_loss: 0.0927 - val_accuracy: 0.9750\n",
      "Epoch 16/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.1061 - accuracy: 0.9588 - val_loss: 0.0893 - val_accuracy: 0.9812\n",
      "Epoch 17/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.1084 - accuracy: 0.9625 - val_loss: 0.0812 - val_accuracy: 0.9812\n",
      "Epoch 18/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.1076 - accuracy: 0.9588 - val_loss: 0.0796 - val_accuracy: 0.9750\n",
      "Epoch 19/500\n",
      "800/800 [==============================] - 0s 608us/step - loss: 0.1092 - accuracy: 0.9588 - val_loss: 0.0746 - val_accuracy: 0.9812\n",
      "Epoch 20/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.1161 - accuracy: 0.9588 - val_loss: 0.0789 - val_accuracy: 0.9812\n",
      "Epoch 21/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0833 - accuracy: 0.9775 - val_loss: 0.0753 - val_accuracy: 0.9812\n",
      "Epoch 22/500\n",
      "800/800 [==============================] - 0s 619us/step - loss: 0.0817 - accuracy: 0.9688 - val_loss: 0.0655 - val_accuracy: 0.9812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0981 - accuracy: 0.9675 - val_loss: 0.0689 - val_accuracy: 0.9750\n",
      "Epoch 24/500\n",
      "800/800 [==============================] - 0s 575us/step - loss: 0.0921 - accuracy: 0.9600 - val_loss: 0.0656 - val_accuracy: 0.9812\n",
      "Epoch 25/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0866 - accuracy: 0.9700 - val_loss: 0.0613 - val_accuracy: 0.9812\n",
      "Epoch 26/500\n",
      "800/800 [==============================] - 0s 572us/step - loss: 0.0621 - accuracy: 0.9787 - val_loss: 0.0615 - val_accuracy: 0.9812\n",
      "Epoch 27/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0728 - accuracy: 0.9750 - val_loss: 0.0616 - val_accuracy: 0.9812\n",
      "Epoch 28/500\n",
      "800/800 [==============================] - 0s 623us/step - loss: 0.0687 - accuracy: 0.9750 - val_loss: 0.0624 - val_accuracy: 0.9812\n",
      "Epoch 29/500\n",
      "800/800 [==============================] - 0s 601us/step - loss: 0.0762 - accuracy: 0.9675 - val_loss: 0.0559 - val_accuracy: 0.9812\n",
      "Epoch 30/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0701 - accuracy: 0.9762 - val_loss: 0.0518 - val_accuracy: 0.9812\n",
      "Epoch 31/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0778 - accuracy: 0.9700 - val_loss: 0.0521 - val_accuracy: 0.9812\n",
      "Epoch 32/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0772 - accuracy: 0.9712 - val_loss: 0.0546 - val_accuracy: 0.9812\n",
      "Epoch 33/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0486 - accuracy: 0.9887 - val_loss: 0.0544 - val_accuracy: 0.9875\n",
      "Epoch 34/500\n",
      "800/800 [==============================] - 0s 596us/step - loss: 0.0712 - accuracy: 0.9737 - val_loss: 0.0500 - val_accuracy: 0.9812\n",
      "Epoch 35/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0618 - accuracy: 0.9762 - val_loss: 0.0463 - val_accuracy: 0.9812\n",
      "Epoch 36/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0623 - accuracy: 0.9750 - val_loss: 0.0441 - val_accuracy: 0.9812\n",
      "Epoch 37/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0639 - accuracy: 0.9712 - val_loss: 0.0487 - val_accuracy: 0.9812\n",
      "Epoch 38/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0681 - accuracy: 0.9725 - val_loss: 0.0501 - val_accuracy: 0.9875\n",
      "Epoch 39/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0658 - accuracy: 0.9750 - val_loss: 0.0551 - val_accuracy: 0.9812\n",
      "Epoch 40/500\n",
      "800/800 [==============================] - 1s 625us/step - loss: 0.0552 - accuracy: 0.9837 - val_loss: 0.0554 - val_accuracy: 0.9812\n",
      "Epoch 41/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0490 - accuracy: 0.9812 - val_loss: 0.0523 - val_accuracy: 0.9812\n",
      "Epoch 42/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0469 - accuracy: 0.9825 - val_loss: 0.0468 - val_accuracy: 0.9875\n",
      "Epoch 43/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0518 - accuracy: 0.9825 - val_loss: 0.0511 - val_accuracy: 0.9750\n",
      "Epoch 44/500\n",
      "800/800 [==============================] - 0s 592us/step - loss: 0.0410 - accuracy: 0.9825 - val_loss: 0.0476 - val_accuracy: 0.9812\n",
      "Epoch 45/500\n",
      "800/800 [==============================] - 1s 627us/step - loss: 0.0380 - accuracy: 0.9875 - val_loss: 0.0433 - val_accuracy: 0.9750\n",
      "Epoch 46/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0405 - accuracy: 0.9912 - val_loss: 0.0476 - val_accuracy: 0.9750\n",
      "Epoch 47/500\n",
      "800/800 [==============================] - 0s 573us/step - loss: 0.0718 - accuracy: 0.9725 - val_loss: 0.0552 - val_accuracy: 0.9750\n",
      "Epoch 48/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0465 - accuracy: 0.9862 - val_loss: 0.0545 - val_accuracy: 0.9812\n",
      "Epoch 49/500\n",
      "800/800 [==============================] - 0s 573us/step - loss: 0.0485 - accuracy: 0.9800 - val_loss: 0.0541 - val_accuracy: 0.9812\n",
      "Epoch 50/500\n",
      "800/800 [==============================] - 0s 572us/step - loss: 0.0351 - accuracy: 0.9862 - val_loss: 0.0518 - val_accuracy: 0.9750\n",
      "Epoch 51/500\n",
      "800/800 [==============================] - 0s 574us/step - loss: 0.0489 - accuracy: 0.9837 - val_loss: 0.0481 - val_accuracy: 0.9812\n",
      "Epoch 52/500\n",
      "800/800 [==============================] - 0s 574us/step - loss: 0.0391 - accuracy: 0.9850 - val_loss: 0.0432 - val_accuracy: 0.9750\n",
      "Epoch 53/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0487 - accuracy: 0.9812 - val_loss: 0.0444 - val_accuracy: 0.9750\n",
      "Epoch 54/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0377 - accuracy: 0.9825 - val_loss: 0.0480 - val_accuracy: 0.9812\n",
      "Epoch 55/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0425 - accuracy: 0.9812 - val_loss: 0.0465 - val_accuracy: 0.9812\n",
      "Epoch 56/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0393 - accuracy: 0.9875 - val_loss: 0.0465 - val_accuracy: 0.9812\n",
      "Epoch 57/500\n",
      "800/800 [==============================] - 0s 572us/step - loss: 0.0528 - accuracy: 0.9787 - val_loss: 0.0398 - val_accuracy: 0.9812\n",
      "Epoch 58/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0356 - accuracy: 0.9862 - val_loss: 0.0382 - val_accuracy: 0.9812\n",
      "Epoch 59/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0316 - accuracy: 0.9937 - val_loss: 0.0411 - val_accuracy: 0.9812\n",
      "Epoch 60/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0220 - accuracy: 0.9975 - val_loss: 0.0463 - val_accuracy: 0.9812\n",
      "Epoch 61/500\n",
      "800/800 [==============================] - 0s 573us/step - loss: 0.0489 - accuracy: 0.9887 - val_loss: 0.0513 - val_accuracy: 0.9812\n",
      "Epoch 62/500\n",
      "800/800 [==============================] - 0s 574us/step - loss: 0.0368 - accuracy: 0.9875 - val_loss: 0.0489 - val_accuracy: 0.9750\n",
      "Epoch 63/500\n",
      "800/800 [==============================] - 0s 575us/step - loss: 0.0281 - accuracy: 0.9875 - val_loss: 0.0585 - val_accuracy: 0.9750\n",
      "Epoch 64/500\n",
      "800/800 [==============================] - 0s 573us/step - loss: 0.0356 - accuracy: 0.9875 - val_loss: 0.0547 - val_accuracy: 0.9750\n",
      "Epoch 65/500\n",
      "800/800 [==============================] - 0s 576us/step - loss: 0.0395 - accuracy: 0.9862 - val_loss: 0.0435 - val_accuracy: 0.9750\n",
      "Epoch 66/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0360 - accuracy: 0.9912 - val_loss: 0.0422 - val_accuracy: 0.9812\n",
      "Epoch 67/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0267 - accuracy: 0.9937 - val_loss: 0.0462 - val_accuracy: 0.9750\n",
      "Epoch 68/500\n",
      "800/800 [==============================] - 0s 576us/step - loss: 0.0337 - accuracy: 0.9825 - val_loss: 0.0413 - val_accuracy: 0.9750\n",
      "Epoch 69/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0393 - accuracy: 0.9862 - val_loss: 0.0388 - val_accuracy: 0.9812\n",
      "Epoch 70/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0601 - accuracy: 0.9837 - val_loss: 0.0400 - val_accuracy: 0.9750\n",
      "Epoch 71/500\n",
      "800/800 [==============================] - 0s 600us/step - loss: 0.0374 - accuracy: 0.9812 - val_loss: 0.0471 - val_accuracy: 0.9812\n",
      "Epoch 72/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0479 - accuracy: 0.9812 - val_loss: 0.0526 - val_accuracy: 0.9812\n",
      "Epoch 73/500\n",
      "800/800 [==============================] - 0s 617us/step - loss: 0.0419 - accuracy: 0.9800 - val_loss: 0.0588 - val_accuracy: 0.9812\n",
      "Epoch 74/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0294 - accuracy: 0.9925 - val_loss: 0.0471 - val_accuracy: 0.9750\n",
      "Epoch 75/500\n",
      "800/800 [==============================] - 0s 575us/step - loss: 0.0316 - accuracy: 0.9900 - val_loss: 0.0482 - val_accuracy: 0.9750\n",
      "Epoch 76/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0407 - accuracy: 0.9875 - val_loss: 0.0464 - val_accuracy: 0.9812\n",
      "Epoch 77/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0228 - accuracy: 0.9900 - val_loss: 0.0430 - val_accuracy: 0.9875\n",
      "Epoch 78/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0404 - accuracy: 0.9812 - val_loss: 0.0430 - val_accuracy: 0.9875\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 584us/step - loss: 0.0311 - accuracy: 0.9900 - val_loss: 0.0344 - val_accuracy: 0.9812\n",
      "Epoch 80/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0258 - accuracy: 0.9925 - val_loss: 0.0339 - val_accuracy: 0.9875\n",
      "Epoch 81/500\n",
      "800/800 [==============================] - 0s 609us/step - loss: 0.0357 - accuracy: 0.9900 - val_loss: 0.0361 - val_accuracy: 0.9875\n",
      "Epoch 82/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0206 - accuracy: 0.9925 - val_loss: 0.0336 - val_accuracy: 0.9750\n",
      "Epoch 83/500\n",
      "800/800 [==============================] - 0s 574us/step - loss: 0.0274 - accuracy: 0.9912 - val_loss: 0.0324 - val_accuracy: 0.9875\n",
      "Epoch 84/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0228 - accuracy: 0.9925 - val_loss: 0.0303 - val_accuracy: 0.9750\n",
      "Epoch 85/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0264 - accuracy: 0.9900 - val_loss: 0.0326 - val_accuracy: 0.9875\n",
      "Epoch 86/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0154 - accuracy: 0.9962 - val_loss: 0.0381 - val_accuracy: 0.9812\n",
      "Epoch 87/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0319 - accuracy: 0.9875 - val_loss: 0.0398 - val_accuracy: 0.9875\n",
      "Epoch 88/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0387 - accuracy: 0.9850 - val_loss: 0.0347 - val_accuracy: 0.9812\n",
      "Epoch 89/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0316 - accuracy: 0.9850 - val_loss: 0.0327 - val_accuracy: 0.9937\n",
      "Epoch 90/500\n",
      "800/800 [==============================] - 0s 575us/step - loss: 0.0213 - accuracy: 0.9912 - val_loss: 0.0325 - val_accuracy: 0.9812\n",
      "Epoch 91/500\n",
      "800/800 [==============================] - 0s 574us/step - loss: 0.0263 - accuracy: 0.9875 - val_loss: 0.0342 - val_accuracy: 0.9875\n",
      "Epoch 92/500\n",
      "800/800 [==============================] - 0s 574us/step - loss: 0.0522 - accuracy: 0.9775 - val_loss: 0.0360 - val_accuracy: 0.9812\n",
      "Epoch 93/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0288 - accuracy: 0.9900 - val_loss: 0.0324 - val_accuracy: 0.9875\n",
      "Epoch 94/500\n",
      "800/800 [==============================] - 0s 573us/step - loss: 0.0192 - accuracy: 0.9925 - val_loss: 0.0289 - val_accuracy: 0.9875\n",
      "Epoch 95/500\n",
      "800/800 [==============================] - 0s 575us/step - loss: 0.0306 - accuracy: 0.9887 - val_loss: 0.0309 - val_accuracy: 0.9937\n",
      "Epoch 96/500\n",
      "800/800 [==============================] - 0s 576us/step - loss: 0.0230 - accuracy: 0.9912 - val_loss: 0.0326 - val_accuracy: 0.9937\n",
      "Epoch 97/500\n",
      "800/800 [==============================] - 0s 573us/step - loss: 0.0184 - accuracy: 0.9950 - val_loss: 0.0284 - val_accuracy: 0.9937\n",
      "Epoch 98/500\n",
      "800/800 [==============================] - 0s 573us/step - loss: 0.0220 - accuracy: 0.9912 - val_loss: 0.0260 - val_accuracy: 0.9875\n",
      "Epoch 99/500\n",
      "800/800 [==============================] - 0s 574us/step - loss: 0.0283 - accuracy: 0.9887 - val_loss: 0.0320 - val_accuracy: 0.9875\n",
      "Epoch 100/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0278 - accuracy: 0.9925 - val_loss: 0.0365 - val_accuracy: 0.9812\n",
      "Epoch 101/500\n",
      "800/800 [==============================] - 1s 667us/step - loss: 0.0275 - accuracy: 0.9875 - val_loss: 0.0389 - val_accuracy: 0.9812\n",
      "Epoch 102/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0184 - accuracy: 0.9937 - val_loss: 0.0418 - val_accuracy: 0.9812\n",
      "Epoch 103/500\n",
      "800/800 [==============================] - 0s 599us/step - loss: 0.0251 - accuracy: 0.9962 - val_loss: 0.0411 - val_accuracy: 0.9812\n",
      "Epoch 104/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0233 - accuracy: 0.9950 - val_loss: 0.0318 - val_accuracy: 0.9812\n",
      "Epoch 105/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0189 - accuracy: 0.9912 - val_loss: 0.0327 - val_accuracy: 0.9750\n",
      "Epoch 106/500\n",
      "800/800 [==============================] - 0s 574us/step - loss: 0.0140 - accuracy: 0.9950 - val_loss: 0.0358 - val_accuracy: 0.9812\n",
      "Epoch 107/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0190 - accuracy: 0.9925 - val_loss: 0.0290 - val_accuracy: 0.9875\n",
      "Epoch 108/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0303 - accuracy: 0.9862 - val_loss: 0.0276 - val_accuracy: 0.9812\n",
      "Epoch 109/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0226 - accuracy: 0.9912 - val_loss: 0.0342 - val_accuracy: 0.9812\n",
      "Epoch 110/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0358 - accuracy: 0.9887 - val_loss: 0.0282 - val_accuracy: 0.9812\n",
      "Epoch 111/500\n",
      "800/800 [==============================] - 0s 576us/step - loss: 0.0284 - accuracy: 0.9925 - val_loss: 0.0248 - val_accuracy: 0.9937\n",
      "Epoch 112/500\n",
      "800/800 [==============================] - 0s 575us/step - loss: 0.0233 - accuracy: 0.9925 - val_loss: 0.0232 - val_accuracy: 0.9875\n",
      "Epoch 113/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0107 - accuracy: 0.9987 - val_loss: 0.0241 - val_accuracy: 0.9937\n",
      "Epoch 114/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0235 - accuracy: 0.9912 - val_loss: 0.0284 - val_accuracy: 0.9875\n",
      "Epoch 115/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0125 - accuracy: 0.9962 - val_loss: 0.0265 - val_accuracy: 0.9875\n",
      "Epoch 116/500\n",
      "800/800 [==============================] - 0s 603us/step - loss: 0.0231 - accuracy: 0.9900 - val_loss: 0.0270 - val_accuracy: 0.9875\n",
      "Epoch 117/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0202 - accuracy: 0.9950 - val_loss: 0.0277 - val_accuracy: 0.9875\n",
      "Epoch 118/500\n",
      "800/800 [==============================] - 1s 628us/step - loss: 0.0161 - accuracy: 0.9925 - val_loss: 0.0302 - val_accuracy: 0.9875\n",
      "Epoch 119/500\n",
      "800/800 [==============================] - 0s 605us/step - loss: 0.0211 - accuracy: 0.9925 - val_loss: 0.0323 - val_accuracy: 0.9812\n",
      "Epoch 120/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0261 - accuracy: 0.9862 - val_loss: 0.0275 - val_accuracy: 0.9812\n",
      "Epoch 121/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0145 - accuracy: 0.9950 - val_loss: 0.0306 - val_accuracy: 0.9812\n",
      "Epoch 122/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0229 - accuracy: 0.9925 - val_loss: 0.0270 - val_accuracy: 0.9812\n",
      "Epoch 123/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0202 - accuracy: 0.9937 - val_loss: 0.0235 - val_accuracy: 0.9875\n",
      "Epoch 124/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0178 - accuracy: 0.9887 - val_loss: 0.0214 - val_accuracy: 0.9937\n",
      "Epoch 125/500\n",
      "800/800 [==============================] - 0s 592us/step - loss: 0.0125 - accuracy: 0.9950 - val_loss: 0.0261 - val_accuracy: 0.9875\n",
      "Epoch 126/500\n",
      "800/800 [==============================] - 0s 574us/step - loss: 0.0097 - accuracy: 0.9987 - val_loss: 0.0260 - val_accuracy: 0.9875\n",
      "Epoch 127/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0172 - accuracy: 0.9962 - val_loss: 0.0233 - val_accuracy: 0.9875\n",
      "Epoch 128/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0236 - accuracy: 0.9937 - val_loss: 0.0228 - val_accuracy: 0.9937\n",
      "Epoch 129/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0110 - accuracy: 0.9962 - val_loss: 0.0239 - val_accuracy: 0.9937\n",
      "Epoch 130/500\n",
      "800/800 [==============================] - 0s 575us/step - loss: 0.0125 - accuracy: 0.9975 - val_loss: 0.0241 - val_accuracy: 0.9937\n",
      "Epoch 131/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0184 - accuracy: 0.9950 - val_loss: 0.0270 - val_accuracy: 0.9875\n",
      "Epoch 132/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0296 - accuracy: 0.9900 - val_loss: 0.0261 - val_accuracy: 0.9875\n",
      "Epoch 133/500\n",
      "800/800 [==============================] - 0s 598us/step - loss: 0.0150 - accuracy: 0.9950 - val_loss: 0.0311 - val_accuracy: 0.9875\n",
      "Epoch 134/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0213 - accuracy: 0.9925 - val_loss: 0.0350 - val_accuracy: 0.9875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/500\n",
      "800/800 [==============================] - 0s 576us/step - loss: 0.0102 - accuracy: 0.9962 - val_loss: 0.0337 - val_accuracy: 0.9875\n",
      "Epoch 136/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0163 - accuracy: 0.9925 - val_loss: 0.0296 - val_accuracy: 0.9937\n",
      "Epoch 137/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0233 - accuracy: 0.9950 - val_loss: 0.0291 - val_accuracy: 0.9875\n",
      "Epoch 138/500\n",
      "800/800 [==============================] - 0s 575us/step - loss: 0.0150 - accuracy: 0.9925 - val_loss: 0.0265 - val_accuracy: 0.9937\n",
      "Epoch 139/500\n",
      "800/800 [==============================] - 0s 576us/step - loss: 0.0184 - accuracy: 0.9912 - val_loss: 0.0280 - val_accuracy: 0.9812\n",
      "Epoch 140/500\n",
      "800/800 [==============================] - 0s 574us/step - loss: 0.0150 - accuracy: 0.9937 - val_loss: 0.0268 - val_accuracy: 0.9812\n",
      "Epoch 141/500\n",
      "800/800 [==============================] - 1s 708us/step - loss: 0.0280 - accuracy: 0.9862 - val_loss: 0.0277 - val_accuracy: 0.9937\n",
      "Epoch 142/500\n",
      "800/800 [==============================] - 0s 617us/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0309 - val_accuracy: 0.9937\n",
      "Epoch 143/500\n",
      "800/800 [==============================] - 1s 680us/step - loss: 0.0232 - accuracy: 0.9925 - val_loss: 0.0321 - val_accuracy: 0.9812\n",
      "Epoch 144/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0197 - accuracy: 0.9950 - val_loss: 0.0302 - val_accuracy: 0.9750\n",
      "Epoch 145/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0132 - accuracy: 0.9975 - val_loss: 0.0270 - val_accuracy: 0.9875\n",
      "Epoch 146/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0310 - accuracy: 0.9887 - val_loss: 0.0254 - val_accuracy: 0.9875\n",
      "Epoch 147/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0184 - accuracy: 0.9937 - val_loss: 0.0264 - val_accuracy: 0.9875\n",
      "Epoch 148/500\n",
      "800/800 [==============================] - 0s 575us/step - loss: 0.0184 - accuracy: 0.9937 - val_loss: 0.0335 - val_accuracy: 0.9875\n",
      "Epoch 149/500\n",
      "800/800 [==============================] - 1s 656us/step - loss: 0.0109 - accuracy: 0.9975 - val_loss: 0.0368 - val_accuracy: 0.9875\n",
      "Epoch 150/500\n",
      "800/800 [==============================] - 0s 594us/step - loss: 0.0243 - accuracy: 0.9900 - val_loss: 0.0331 - val_accuracy: 0.9875\n",
      "Epoch 151/500\n",
      "800/800 [==============================] - 0s 604us/step - loss: 0.0165 - accuracy: 0.9937 - val_loss: 0.0370 - val_accuracy: 0.9875\n",
      "Epoch 152/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0210 - accuracy: 0.9925 - val_loss: 0.0327 - val_accuracy: 0.9875\n",
      "Epoch 153/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0177 - accuracy: 0.9912 - val_loss: 0.0235 - val_accuracy: 0.9937\n",
      "Epoch 154/500\n",
      "800/800 [==============================] - 1s 637us/step - loss: 0.0128 - accuracy: 0.9937 - val_loss: 0.0294 - val_accuracy: 0.9875\n",
      "Epoch 155/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0113 - accuracy: 0.9975 - val_loss: 0.0221 - val_accuracy: 0.9937\n",
      "Epoch 156/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0133 - accuracy: 0.9937 - val_loss: 0.0215 - val_accuracy: 0.9937\n",
      "Epoch 157/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0142 - accuracy: 0.9937 - val_loss: 0.0284 - val_accuracy: 0.9875\n",
      "Epoch 158/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0300 - accuracy: 0.9887 - val_loss: 0.0393 - val_accuracy: 0.9812\n",
      "Epoch 159/500\n",
      "800/800 [==============================] - 0s 592us/step - loss: 0.0195 - accuracy: 0.9950 - val_loss: 0.0451 - val_accuracy: 0.9750\n",
      "Epoch 160/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0193 - accuracy: 0.9937 - val_loss: 0.0377 - val_accuracy: 0.9875\n",
      "Epoch 161/500\n",
      "800/800 [==============================] - 0s 610us/step - loss: 0.0269 - accuracy: 0.9912 - val_loss: 0.0364 - val_accuracy: 0.9875\n",
      "Epoch 162/500\n",
      "800/800 [==============================] - 0s 614us/step - loss: 0.0309 - accuracy: 0.9887 - val_loss: 0.0241 - val_accuracy: 0.9937\n",
      "Epoch 163/500\n",
      "800/800 [==============================] - 0s 592us/step - loss: 0.0069 - accuracy: 0.9987 - val_loss: 0.0243 - val_accuracy: 0.9937\n",
      "Epoch 164/500\n",
      "800/800 [==============================] - 0s 611us/step - loss: 0.0117 - accuracy: 0.9975 - val_loss: 0.0232 - val_accuracy: 0.9937\n",
      "Epoch 165/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0121 - accuracy: 0.9950 - val_loss: 0.0221 - val_accuracy: 0.9937\n",
      "Epoch 166/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0237 - accuracy: 0.9900 - val_loss: 0.0204 - val_accuracy: 0.9937\n",
      "Epoch 167/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0104 - accuracy: 0.9975 - val_loss: 0.0229 - val_accuracy: 0.9937\n",
      "Epoch 168/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0062 - accuracy: 0.9975 - val_loss: 0.0255 - val_accuracy: 0.9937\n",
      "Epoch 169/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0111 - accuracy: 0.9962 - val_loss: 0.0240 - val_accuracy: 0.9937\n",
      "Epoch 170/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.0240 - val_accuracy: 0.9875\n",
      "Epoch 171/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0072 - accuracy: 0.9987 - val_loss: 0.0260 - val_accuracy: 0.9875\n",
      "Epoch 172/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0135 - accuracy: 0.9912 - val_loss: 0.0220 - val_accuracy: 0.9875\n",
      "Epoch 173/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0070 - accuracy: 0.9975 - val_loss: 0.0210 - val_accuracy: 0.9937\n",
      "Epoch 174/500\n",
      "800/800 [==============================] - 0s 600us/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 0.9875\n",
      "Epoch 175/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0179 - accuracy: 0.9925 - val_loss: 0.0214 - val_accuracy: 0.9937\n",
      "Epoch 176/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0188 - accuracy: 0.9925 - val_loss: 0.0213 - val_accuracy: 0.9937\n",
      "Epoch 177/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.0255 - val_accuracy: 0.9937\n",
      "Epoch 178/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0154 - accuracy: 0.9937 - val_loss: 0.0295 - val_accuracy: 0.9812\n",
      "Epoch 179/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0156 - accuracy: 0.9912 - val_loss: 0.0450 - val_accuracy: 0.9812\n",
      "Epoch 180/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0168 - accuracy: 0.9950 - val_loss: 0.0449 - val_accuracy: 0.9750\n",
      "Epoch 181/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0420 - val_accuracy: 0.9812\n",
      "Epoch 182/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0165 - accuracy: 0.9912 - val_loss: 0.0393 - val_accuracy: 0.9812\n",
      "Epoch 183/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0212 - accuracy: 0.9937 - val_loss: 0.0393 - val_accuracy: 0.9812\n",
      "Epoch 184/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0128 - accuracy: 0.9975 - val_loss: 0.0439 - val_accuracy: 0.9812\n",
      "Epoch 185/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0138 - accuracy: 0.9962 - val_loss: 0.0370 - val_accuracy: 0.9812\n",
      "Epoch 186/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0349 - accuracy: 0.9875 - val_loss: 0.0361 - val_accuracy: 0.9812\n",
      "Epoch 187/500\n",
      "800/800 [==============================] - 0s 604us/step - loss: 0.0155 - accuracy: 0.9950 - val_loss: 0.0392 - val_accuracy: 0.9875\n",
      "Epoch 188/500\n",
      "800/800 [==============================] - 0s 595us/step - loss: 0.0187 - accuracy: 0.9962 - val_loss: 0.0334 - val_accuracy: 0.9875\n",
      "Epoch 189/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0157 - accuracy: 0.9937 - val_loss: 0.0287 - val_accuracy: 0.9875\n",
      "Epoch 190/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0143 - accuracy: 0.9950 - val_loss: 0.0339 - val_accuracy: 0.9875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0090 - accuracy: 0.9962 - val_loss: 0.0375 - val_accuracy: 0.9875\n",
      "Epoch 192/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0235 - accuracy: 0.9912 - val_loss: 0.0326 - val_accuracy: 0.9937\n",
      "Epoch 193/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.0346 - val_accuracy: 0.9937\n",
      "Epoch 194/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0356 - val_accuracy: 0.9875\n",
      "Epoch 195/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0125 - accuracy: 0.9950 - val_loss: 0.0373 - val_accuracy: 0.9875\n",
      "Epoch 196/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.0359 - val_accuracy: 0.9875\n",
      "Epoch 197/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0098 - accuracy: 0.9975 - val_loss: 0.0336 - val_accuracy: 0.9875\n",
      "Epoch 198/500\n",
      "800/800 [==============================] - 0s 594us/step - loss: 0.0126 - accuracy: 0.9962 - val_loss: 0.0373 - val_accuracy: 0.9812\n",
      "Epoch 199/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.0371 - val_accuracy: 0.9875\n",
      "Epoch 200/500\n",
      "800/800 [==============================] - 1s 649us/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.0345 - val_accuracy: 0.9875\n",
      "Epoch 201/500\n",
      "800/800 [==============================] - 0s 608us/step - loss: 0.0231 - accuracy: 0.9925 - val_loss: 0.0272 - val_accuracy: 0.9875\n",
      "Epoch 202/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0130 - accuracy: 0.9975 - val_loss: 0.0261 - val_accuracy: 0.9937\n",
      "Epoch 203/500\n",
      "800/800 [==============================] - 0s 607us/step - loss: 0.0237 - accuracy: 0.9887 - val_loss: 0.0254 - val_accuracy: 0.9937\n",
      "Epoch 204/500\n",
      "800/800 [==============================] - 1s 648us/step - loss: 0.0096 - accuracy: 0.9937 - val_loss: 0.0223 - val_accuracy: 0.9937\n",
      "Epoch 205/500\n",
      "800/800 [==============================] - 1s 782us/step - loss: 0.0091 - accuracy: 0.9962 - val_loss: 0.0205 - val_accuracy: 0.9937\n",
      "Epoch 206/500\n",
      "800/800 [==============================] - 1s 645us/step - loss: 0.0125 - accuracy: 0.9950 - val_loss: 0.0239 - val_accuracy: 0.9875\n",
      "Epoch 207/500\n",
      "800/800 [==============================] - 0s 605us/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 0.0267 - val_accuracy: 0.9875\n",
      "Epoch 208/500\n",
      "800/800 [==============================] - 0s 596us/step - loss: 0.0092 - accuracy: 0.9950 - val_loss: 0.0248 - val_accuracy: 0.9875\n",
      "Epoch 209/500\n",
      "800/800 [==============================] - 1s 629us/step - loss: 0.0109 - accuracy: 0.9962 - val_loss: 0.0313 - val_accuracy: 0.9812\n",
      "Epoch 210/500\n",
      "800/800 [==============================] - 0s 607us/step - loss: 0.0167 - accuracy: 0.9937 - val_loss: 0.0334 - val_accuracy: 0.9875\n",
      "Epoch 211/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0126 - accuracy: 0.9950 - val_loss: 0.0466 - val_accuracy: 0.9812\n",
      "Epoch 212/500\n",
      "800/800 [==============================] - 0s 595us/step - loss: 0.0305 - accuracy: 0.9875 - val_loss: 0.0442 - val_accuracy: 0.9812\n",
      "Epoch 213/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0154 - accuracy: 0.9937 - val_loss: 0.0364 - val_accuracy: 0.9875\n",
      "Epoch 214/500\n",
      "800/800 [==============================] - 1s 860us/step - loss: 0.0125 - accuracy: 0.9962 - val_loss: 0.0307 - val_accuracy: 0.9812\n",
      "Epoch 215/500\n",
      "800/800 [==============================] - 1s 626us/step - loss: 0.0225 - accuracy: 0.9925 - val_loss: 0.0317 - val_accuracy: 0.9812\n",
      "Epoch 216/500\n",
      "800/800 [==============================] - 0s 597us/step - loss: 0.0062 - accuracy: 0.9987 - val_loss: 0.0289 - val_accuracy: 0.9875\n",
      "Epoch 217/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0109 - accuracy: 0.9975 - val_loss: 0.0341 - val_accuracy: 0.9875\n",
      "Epoch 218/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0148 - accuracy: 0.9950 - val_loss: 0.0341 - val_accuracy: 0.9812\n",
      "Epoch 219/500\n",
      "800/800 [==============================] - 0s 607us/step - loss: 0.0135 - accuracy: 0.9950 - val_loss: 0.0363 - val_accuracy: 0.9812\n",
      "Epoch 220/500\n",
      "800/800 [==============================] - 0s 600us/step - loss: 0.0157 - accuracy: 0.9962 - val_loss: 0.0310 - val_accuracy: 0.9875\n",
      "Epoch 221/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0237 - accuracy: 0.9875 - val_loss: 0.0345 - val_accuracy: 0.9875\n",
      "Epoch 222/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0183 - accuracy: 0.9950 - val_loss: 0.0273 - val_accuracy: 0.9812\n",
      "Epoch 223/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0151 - accuracy: 0.9925 - val_loss: 0.0279 - val_accuracy: 0.9812\n",
      "Epoch 224/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0102 - accuracy: 0.9962 - val_loss: 0.0338 - val_accuracy: 0.9812\n",
      "Epoch 225/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0126 - accuracy: 0.9975 - val_loss: 0.0350 - val_accuracy: 0.9812\n",
      "Epoch 226/500\n",
      "800/800 [==============================] - 0s 592us/step - loss: 0.0192 - accuracy: 0.9937 - val_loss: 0.0302 - val_accuracy: 0.9812\n",
      "Epoch 227/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0109 - accuracy: 0.9962 - val_loss: 0.0294 - val_accuracy: 0.9812\n",
      "Epoch 228/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0081 - accuracy: 0.9962 - val_loss: 0.0287 - val_accuracy: 0.9875\n",
      "Epoch 229/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0149 - accuracy: 0.9937 - val_loss: 0.0252 - val_accuracy: 0.9812\n",
      "Epoch 230/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0079 - accuracy: 0.9962 - val_loss: 0.0284 - val_accuracy: 0.9812\n",
      "Epoch 231/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0118 - accuracy: 0.9950 - val_loss: 0.0252 - val_accuracy: 0.9812\n",
      "Epoch 232/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0208 - accuracy: 0.9937 - val_loss: 0.0178 - val_accuracy: 0.9937\n",
      "Epoch 233/500\n",
      "800/800 [==============================] - 0s 600us/step - loss: 0.0124 - accuracy: 0.9962 - val_loss: 0.0192 - val_accuracy: 0.9937\n",
      "Epoch 234/500\n",
      "800/800 [==============================] - 0s 609us/step - loss: 0.0068 - accuracy: 0.9962 - val_loss: 0.0188 - val_accuracy: 0.9937\n",
      "Epoch 235/500\n",
      "800/800 [==============================] - 0s 594us/step - loss: 0.0125 - accuracy: 0.9937 - val_loss: 0.0259 - val_accuracy: 0.9937\n",
      "Epoch 236/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0096 - accuracy: 0.9962 - val_loss: 0.0298 - val_accuracy: 0.9937\n",
      "Epoch 237/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0179 - accuracy: 0.9937 - val_loss: 0.0355 - val_accuracy: 0.9875\n",
      "Epoch 238/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 0.0401 - val_accuracy: 0.9812\n",
      "Epoch 239/500\n",
      "800/800 [==============================] - 0s 595us/step - loss: 0.0208 - accuracy: 0.9925 - val_loss: 0.0411 - val_accuracy: 0.9750\n",
      "Epoch 240/500\n",
      "800/800 [==============================] - 0s 595us/step - loss: 0.0119 - accuracy: 0.9950 - val_loss: 0.0442 - val_accuracy: 0.9812\n",
      "Epoch 241/500\n",
      "800/800 [==============================] - 0s 614us/step - loss: 0.0155 - accuracy: 0.9937 - val_loss: 0.0374 - val_accuracy: 0.9750\n",
      "Epoch 242/500\n",
      "800/800 [==============================] - 0s 600us/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0358 - val_accuracy: 0.9812\n",
      "Epoch 243/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.0351 - val_accuracy: 0.9812\n",
      "Epoch 244/500\n",
      "800/800 [==============================] - 0s 597us/step - loss: 0.0194 - accuracy: 0.9925 - val_loss: 0.0338 - val_accuracy: 0.9812\n",
      "Epoch 245/500\n",
      "800/800 [==============================] - 0s 595us/step - loss: 0.0068 - accuracy: 0.9975 - val_loss: 0.0337 - val_accuracy: 0.9812\n",
      "Epoch 246/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0087 - accuracy: 0.9975 - val_loss: 0.0356 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0149 - accuracy: 0.9950 - val_loss: 0.0359 - val_accuracy: 0.9875\n",
      "Epoch 248/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0116 - accuracy: 0.9937 - val_loss: 0.0396 - val_accuracy: 0.9812\n",
      "Epoch 249/500\n",
      "800/800 [==============================] - 0s 599us/step - loss: 0.0253 - accuracy: 0.9912 - val_loss: 0.0339 - val_accuracy: 0.9812\n",
      "Epoch 250/500\n",
      "800/800 [==============================] - 0s 618us/step - loss: 0.0053 - accuracy: 0.9975 - val_loss: 0.0328 - val_accuracy: 0.9875\n",
      "Epoch 251/500\n",
      "800/800 [==============================] - 0s 597us/step - loss: 0.0156 - accuracy: 0.9950 - val_loss: 0.0293 - val_accuracy: 0.9812\n",
      "Epoch 252/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0341 - accuracy: 0.9875 - val_loss: 0.0252 - val_accuracy: 0.9812\n",
      "Epoch 253/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0056 - accuracy: 0.9975 - val_loss: 0.0282 - val_accuracy: 0.9875\n",
      "Epoch 254/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0160 - accuracy: 0.9937 - val_loss: 0.0314 - val_accuracy: 0.9937\n",
      "Epoch 255/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 0.9937\n",
      "Epoch 256/500\n",
      "800/800 [==============================] - 0s 605us/step - loss: 0.0269 - accuracy: 0.9912 - val_loss: 0.0334 - val_accuracy: 0.9875\n",
      "Epoch 257/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0140 - accuracy: 0.9950 - val_loss: 0.0331 - val_accuracy: 0.9875\n",
      "Epoch 258/500\n",
      "800/800 [==============================] - 0s 619us/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.0328 - val_accuracy: 0.9875\n",
      "Epoch 259/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0108 - accuracy: 0.9950 - val_loss: 0.0346 - val_accuracy: 0.9812\n",
      "Epoch 260/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0134 - accuracy: 0.9962 - val_loss: 0.0410 - val_accuracy: 0.9875\n",
      "Epoch 261/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0145 - accuracy: 0.9937 - val_loss: 0.0365 - val_accuracy: 0.9812\n",
      "Epoch 262/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0128 - accuracy: 0.9950 - val_loss: 0.0304 - val_accuracy: 0.9875\n",
      "Epoch 263/500\n",
      "800/800 [==============================] - 0s 600us/step - loss: 0.0171 - accuracy: 0.9962 - val_loss: 0.0444 - val_accuracy: 0.9875\n",
      "Epoch 264/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0182 - accuracy: 0.9912 - val_loss: 0.0465 - val_accuracy: 0.9875\n",
      "Epoch 265/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0110 - accuracy: 0.9975 - val_loss: 0.0319 - val_accuracy: 0.9875\n",
      "Epoch 266/500\n",
      "800/800 [==============================] - 0s 598us/step - loss: 0.0140 - accuracy: 0.9962 - val_loss: 0.0365 - val_accuracy: 0.9875\n",
      "Epoch 267/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0143 - accuracy: 0.9962 - val_loss: 0.0309 - val_accuracy: 0.9937\n",
      "Epoch 268/500\n",
      "800/800 [==============================] - 1s 661us/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.0309 - val_accuracy: 0.9875\n",
      "Epoch 269/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.0249 - val_accuracy: 0.9875\n",
      "Epoch 270/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.0295 - val_accuracy: 0.9812\n",
      "Epoch 271/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0099 - accuracy: 0.9950 - val_loss: 0.0376 - val_accuracy: 0.9750\n",
      "Epoch 272/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0104 - accuracy: 0.9950 - val_loss: 0.0339 - val_accuracy: 0.9750\n",
      "Epoch 273/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0108 - accuracy: 0.9962 - val_loss: 0.0321 - val_accuracy: 0.9812\n",
      "Epoch 274/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0127 - accuracy: 0.9962 - val_loss: 0.0357 - val_accuracy: 0.9812\n",
      "Epoch 275/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0094 - accuracy: 0.9950 - val_loss: 0.0259 - val_accuracy: 0.9875\n",
      "Epoch 276/500\n",
      "800/800 [==============================] - 0s 592us/step - loss: 0.0242 - accuracy: 0.9975 - val_loss: 0.0235 - val_accuracy: 0.9875\n",
      "Epoch 277/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0218 - val_accuracy: 0.9875\n",
      "Epoch 278/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0180 - accuracy: 0.9937 - val_loss: 0.0235 - val_accuracy: 0.9937\n",
      "Epoch 279/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0084 - accuracy: 0.9962 - val_loss: 0.0226 - val_accuracy: 0.9812\n",
      "Epoch 280/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0094 - accuracy: 0.9962 - val_loss: 0.0201 - val_accuracy: 0.9875\n",
      "Epoch 281/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0095 - accuracy: 0.9962 - val_loss: 0.0202 - val_accuracy: 0.9875\n",
      "Epoch 282/500\n",
      "800/800 [==============================] - 0s 596us/step - loss: 0.0119 - accuracy: 0.9962 - val_loss: 0.0227 - val_accuracy: 0.9875\n",
      "Epoch 283/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0063 - accuracy: 0.9962 - val_loss: 0.0263 - val_accuracy: 0.9875\n",
      "Epoch 284/500\n",
      "800/800 [==============================] - 0s 614us/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.0251 - val_accuracy: 0.9937\n",
      "Epoch 285/500\n",
      "800/800 [==============================] - 0s 592us/step - loss: 0.0107 - accuracy: 0.9950 - val_loss: 0.0282 - val_accuracy: 0.9937\n",
      "Epoch 286/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0171 - accuracy: 0.9912 - val_loss: 0.0288 - val_accuracy: 0.9937\n",
      "Epoch 287/500\n",
      "800/800 [==============================] - 0s 599us/step - loss: 0.0107 - accuracy: 0.9975 - val_loss: 0.0354 - val_accuracy: 0.9812\n",
      "Epoch 288/500\n",
      "800/800 [==============================] - 1s 674us/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.0386 - val_accuracy: 0.9750\n",
      "Epoch 289/500\n",
      "800/800 [==============================] - 1s 627us/step - loss: 0.0067 - accuracy: 0.9987 - val_loss: 0.0354 - val_accuracy: 0.9875\n",
      "Epoch 290/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0052 - accuracy: 0.9975 - val_loss: 0.0358 - val_accuracy: 0.9812\n",
      "Epoch 291/500\n",
      "800/800 [==============================] - 0s 603us/step - loss: 0.0060 - accuracy: 0.9962 - val_loss: 0.0381 - val_accuracy: 0.9750\n",
      "Epoch 292/500\n",
      "800/800 [==============================] - 0s 598us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 0.9750\n",
      "Epoch 293/500\n",
      "800/800 [==============================] - 0s 623us/step - loss: 0.0082 - accuracy: 0.9950 - val_loss: 0.0460 - val_accuracy: 0.9750\n",
      "Epoch 294/500\n",
      "800/800 [==============================] - 1s 627us/step - loss: 0.0105 - accuracy: 0.9975 - val_loss: 0.0531 - val_accuracy: 0.9750\n",
      "Epoch 295/500\n",
      "800/800 [==============================] - 0s 609us/step - loss: 0.0065 - accuracy: 0.9975 - val_loss: 0.0395 - val_accuracy: 0.9812\n",
      "Epoch 296/500\n",
      "800/800 [==============================] - 0s 601us/step - loss: 0.0178 - accuracy: 0.9937 - val_loss: 0.0435 - val_accuracy: 0.9875\n",
      "Epoch 297/500\n",
      "800/800 [==============================] - 0s 596us/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0445 - val_accuracy: 0.9750\n",
      "Epoch 298/500\n",
      "800/800 [==============================] - 0s 596us/step - loss: 0.0162 - accuracy: 0.9950 - val_loss: 0.0327 - val_accuracy: 0.9812\n",
      "Epoch 299/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0098 - accuracy: 0.9975 - val_loss: 0.0282 - val_accuracy: 0.9812\n",
      "Epoch 300/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0083 - accuracy: 0.9962 - val_loss: 0.0302 - val_accuracy: 0.9875\n",
      "Epoch 301/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.0276 - val_accuracy: 0.9812\n",
      "Epoch 302/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 0.0294 - val_accuracy: 0.9875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 303/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0297 - val_accuracy: 0.9812\n",
      "Epoch 304/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0295 - val_accuracy: 0.9812\n",
      "Epoch 305/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0055 - accuracy: 0.9975 - val_loss: 0.0374 - val_accuracy: 0.9750\n",
      "Epoch 306/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0089 - accuracy: 0.9962 - val_loss: 0.0343 - val_accuracy: 0.9750\n",
      "Epoch 307/500\n",
      "800/800 [==============================] - 1s 632us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0342 - val_accuracy: 0.9750\n",
      "Epoch 308/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0089 - accuracy: 0.9962 - val_loss: 0.0398 - val_accuracy: 0.9750\n",
      "Epoch 309/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0106 - accuracy: 0.9950 - val_loss: 0.0436 - val_accuracy: 0.9812\n",
      "Epoch 310/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0057 - accuracy: 0.9987 - val_loss: 0.0440 - val_accuracy: 0.9812\n",
      "Epoch 311/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0133 - accuracy: 0.9962 - val_loss: 0.0352 - val_accuracy: 0.9750\n",
      "Epoch 312/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0285 - val_accuracy: 0.9750\n",
      "Epoch 313/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0063 - accuracy: 0.9975 - val_loss: 0.0286 - val_accuracy: 0.9750\n",
      "Epoch 314/500\n",
      "800/800 [==============================] - 0s 598us/step - loss: 0.0195 - accuracy: 0.9950 - val_loss: 0.0314 - val_accuracy: 0.9750\n",
      "Epoch 315/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 0.0340 - val_accuracy: 0.9750\n",
      "Epoch 316/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.0295 - val_accuracy: 0.9750\n",
      "Epoch 317/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0080 - accuracy: 0.9962 - val_loss: 0.0313 - val_accuracy: 0.9750\n",
      "Epoch 318/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0067 - accuracy: 0.9962 - val_loss: 0.0309 - val_accuracy: 0.9812\n",
      "Epoch 319/500\n",
      "800/800 [==============================] - 0s 607us/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0350 - val_accuracy: 0.9812\n",
      "Epoch 320/500\n",
      "800/800 [==============================] - 0s 597us/step - loss: 0.0278 - accuracy: 0.9912 - val_loss: 0.0391 - val_accuracy: 0.9875\n",
      "Epoch 321/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 0.0471 - val_accuracy: 0.9875\n",
      "Epoch 322/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.0404 - val_accuracy: 0.9875\n",
      "Epoch 323/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 0.9875\n",
      "Epoch 324/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0067 - accuracy: 0.9987 - val_loss: 0.0311 - val_accuracy: 0.9812\n",
      "Epoch 325/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.0284 - val_accuracy: 0.9812\n",
      "Epoch 326/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0178 - accuracy: 0.9937 - val_loss: 0.0253 - val_accuracy: 0.9812\n",
      "Epoch 327/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0070 - accuracy: 0.9950 - val_loss: 0.0244 - val_accuracy: 0.9812\n",
      "Epoch 328/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0067 - accuracy: 0.9987 - val_loss: 0.0259 - val_accuracy: 0.9875\n",
      "Epoch 329/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0140 - accuracy: 0.9925 - val_loss: 0.0259 - val_accuracy: 0.9875\n",
      "Epoch 330/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0204 - accuracy: 0.9925 - val_loss: 0.0337 - val_accuracy: 0.9812\n",
      "Epoch 331/500\n",
      "800/800 [==============================] - 0s 608us/step - loss: 0.0193 - accuracy: 0.9937 - val_loss: 0.0214 - val_accuracy: 0.9875\n",
      "Epoch 332/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0203 - accuracy: 0.9925 - val_loss: 0.0155 - val_accuracy: 0.9937\n",
      "Epoch 333/500\n",
      "800/800 [==============================] - 0s 615us/step - loss: 0.0103 - accuracy: 0.9950 - val_loss: 0.0272 - val_accuracy: 0.9937\n",
      "Epoch 334/500\n",
      "800/800 [==============================] - 1s 708us/step - loss: 0.0066 - accuracy: 0.9962 - val_loss: 0.0340 - val_accuracy: 0.9750\n",
      "Epoch 335/500\n",
      "800/800 [==============================] - 0s 608us/step - loss: 0.0085 - accuracy: 0.9950 - val_loss: 0.0360 - val_accuracy: 0.9812\n",
      "Epoch 336/500\n",
      "800/800 [==============================] - 0s 597us/step - loss: 0.0081 - accuracy: 0.9962 - val_loss: 0.0446 - val_accuracy: 0.9750\n",
      "Epoch 337/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0054 - accuracy: 0.9975 - val_loss: 0.0480 - val_accuracy: 0.9812\n",
      "Epoch 338/500\n",
      "800/800 [==============================] - 0s 618us/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.0430 - val_accuracy: 0.9812\n",
      "Epoch 339/500\n",
      "800/800 [==============================] - 0s 608us/step - loss: 0.0213 - accuracy: 0.9900 - val_loss: 0.0333 - val_accuracy: 0.9812\n",
      "Epoch 340/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0031 - accuracy: 0.9987 - val_loss: 0.0325 - val_accuracy: 0.9875\n",
      "Epoch 341/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0180 - accuracy: 0.9925 - val_loss: 0.0282 - val_accuracy: 0.9875\n",
      "Epoch 342/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0238 - accuracy: 0.9925 - val_loss: 0.0402 - val_accuracy: 0.9812\n",
      "Epoch 343/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0087 - accuracy: 0.9962 - val_loss: 0.0417 - val_accuracy: 0.9812\n",
      "Epoch 344/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0085 - accuracy: 0.9950 - val_loss: 0.0398 - val_accuracy: 0.9812\n",
      "Epoch 345/500\n",
      "800/800 [==============================] - 0s 595us/step - loss: 0.0159 - accuracy: 0.9925 - val_loss: 0.0491 - val_accuracy: 0.9812\n",
      "Epoch 346/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0088 - accuracy: 0.9962 - val_loss: 0.0492 - val_accuracy: 0.9812\n",
      "Epoch 347/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0108 - accuracy: 0.9975 - val_loss: 0.0551 - val_accuracy: 0.9750\n",
      "Epoch 348/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.0540 - val_accuracy: 0.9812\n",
      "Epoch 349/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.9750\n",
      "Epoch 350/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0066 - accuracy: 0.9975 - val_loss: 0.0477 - val_accuracy: 0.9750\n",
      "Epoch 351/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0057 - accuracy: 0.9987 - val_loss: 0.0472 - val_accuracy: 0.9750\n",
      "Epoch 352/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.0495 - val_accuracy: 0.9750\n",
      "Epoch 353/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0247 - accuracy: 0.9900 - val_loss: 0.0402 - val_accuracy: 0.9812\n",
      "Epoch 354/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0064 - accuracy: 0.9975 - val_loss: 0.0375 - val_accuracy: 0.9750\n",
      "Epoch 355/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0176 - accuracy: 0.9937 - val_loss: 0.0415 - val_accuracy: 0.9750\n",
      "Epoch 356/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0512 - val_accuracy: 0.9688\n",
      "Epoch 357/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.0516 - val_accuracy: 0.9750\n",
      "Epoch 358/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0074 - accuracy: 0.9962 - val_loss: 0.0504 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 359/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0478 - val_accuracy: 0.9750\n",
      "Epoch 360/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0094 - accuracy: 0.9987 - val_loss: 0.0416 - val_accuracy: 0.9750\n",
      "Epoch 361/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0070 - accuracy: 0.9975 - val_loss: 0.0402 - val_accuracy: 0.9688\n",
      "Epoch 362/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0087 - accuracy: 0.9975 - val_loss: 0.0427 - val_accuracy: 0.9750\n",
      "Epoch 363/500\n",
      "800/800 [==============================] - 0s 610us/step - loss: 0.0114 - accuracy: 0.9950 - val_loss: 0.0362 - val_accuracy: 0.9812\n",
      "Epoch 364/500\n",
      "800/800 [==============================] - 0s 610us/step - loss: 0.0085 - accuracy: 0.9975 - val_loss: 0.0371 - val_accuracy: 0.9812\n",
      "Epoch 365/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0470 - val_accuracy: 0.9812\n",
      "Epoch 366/500\n",
      "800/800 [==============================] - 0s 594us/step - loss: 0.0077 - accuracy: 0.9962 - val_loss: 0.0500 - val_accuracy: 0.9812\n",
      "Epoch 367/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0164 - accuracy: 0.9925 - val_loss: 0.0532 - val_accuracy: 0.9812\n",
      "Epoch 368/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0124 - accuracy: 0.9950 - val_loss: 0.0444 - val_accuracy: 0.9812\n",
      "Epoch 369/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0128 - accuracy: 0.9925 - val_loss: 0.0416 - val_accuracy: 0.9812\n",
      "Epoch 370/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0156 - accuracy: 0.9950 - val_loss: 0.0344 - val_accuracy: 0.9812\n",
      "Epoch 371/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0086 - accuracy: 0.9987 - val_loss: 0.0303 - val_accuracy: 0.9812\n",
      "Epoch 372/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0102 - accuracy: 0.9950 - val_loss: 0.0342 - val_accuracy: 0.9812\n",
      "Epoch 373/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0297 - val_accuracy: 0.9812\n",
      "Epoch 374/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0089 - accuracy: 0.9962 - val_loss: 0.0335 - val_accuracy: 0.9688\n",
      "Epoch 375/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0070 - accuracy: 0.9975 - val_loss: 0.0370 - val_accuracy: 0.9750\n",
      "Epoch 376/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0071 - accuracy: 0.9987 - val_loss: 0.0387 - val_accuracy: 0.9750\n",
      "Epoch 377/500\n",
      "800/800 [==============================] - 0s 607us/step - loss: 0.0180 - accuracy: 0.9975 - val_loss: 0.0321 - val_accuracy: 0.9812\n",
      "Epoch 378/500\n",
      "800/800 [==============================] - 0s 604us/step - loss: 0.0151 - accuracy: 0.9950 - val_loss: 0.0300 - val_accuracy: 0.9875\n",
      "Epoch 379/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0135 - accuracy: 0.9937 - val_loss: 0.0228 - val_accuracy: 0.9937\n",
      "Epoch 380/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0109 - accuracy: 0.9950 - val_loss: 0.0192 - val_accuracy: 0.9937\n",
      "Epoch 381/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.0229 - val_accuracy: 0.9875\n",
      "Epoch 382/500\n",
      "800/800 [==============================] - 0s 602us/step - loss: 0.0140 - accuracy: 0.9950 - val_loss: 0.0319 - val_accuracy: 0.9812\n",
      "Epoch 383/500\n",
      "800/800 [==============================] - 0s 616us/step - loss: 0.0109 - accuracy: 0.9950 - val_loss: 0.0379 - val_accuracy: 0.9812\n",
      "Epoch 384/500\n",
      "800/800 [==============================] - 0s 598us/step - loss: 0.0166 - accuracy: 0.9962 - val_loss: 0.0483 - val_accuracy: 0.9750\n",
      "Epoch 385/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.0504 - val_accuracy: 0.9750\n",
      "Epoch 386/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0046 - accuracy: 0.9975 - val_loss: 0.0472 - val_accuracy: 0.9688\n",
      "Epoch 387/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0054 - accuracy: 0.9975 - val_loss: 0.0407 - val_accuracy: 0.9812\n",
      "Epoch 388/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0405 - val_accuracy: 0.9812\n",
      "Epoch 389/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0061 - accuracy: 0.9987 - val_loss: 0.0393 - val_accuracy: 0.9875\n",
      "Epoch 390/500\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.0358 - val_accuracy: 0.9750\n",
      "Epoch 391/500\n",
      "800/800 [==============================] - 1s 726us/step - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.0364 - val_accuracy: 0.9812\n",
      "Epoch 392/500\n",
      "800/800 [==============================] - 0s 615us/step - loss: 0.0101 - accuracy: 0.9950 - val_loss: 0.0276 - val_accuracy: 0.9875\n",
      "Epoch 393/500\n",
      "800/800 [==============================] - 1s 813us/step - loss: 0.0135 - accuracy: 0.9962 - val_loss: 0.0241 - val_accuracy: 0.9875\n",
      "Epoch 394/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0068 - accuracy: 0.9962 - val_loss: 0.0219 - val_accuracy: 0.9937\n",
      "Epoch 395/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.0218 - val_accuracy: 0.9937\n",
      "Epoch 396/500\n",
      "800/800 [==============================] - 0s 616us/step - loss: 0.0073 - accuracy: 0.9962 - val_loss: 0.0203 - val_accuracy: 0.9937\n",
      "Epoch 397/500\n",
      "800/800 [==============================] - 0s 602us/step - loss: 0.0105 - accuracy: 0.9950 - val_loss: 0.0201 - val_accuracy: 0.9937\n",
      "Epoch 398/500\n",
      "800/800 [==============================] - 0s 600us/step - loss: 0.0069 - accuracy: 0.9962 - val_loss: 0.0226 - val_accuracy: 0.9875\n",
      "Epoch 399/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0098 - accuracy: 0.9962 - val_loss: 0.0289 - val_accuracy: 0.9875\n",
      "Epoch 400/500\n",
      "800/800 [==============================] - 1s 750us/step - loss: 0.0138 - accuracy: 0.9962 - val_loss: 0.0365 - val_accuracy: 0.9875\n",
      "Epoch 401/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0116 - accuracy: 0.9950 - val_loss: 0.0256 - val_accuracy: 0.9875\n",
      "Epoch 402/500\n",
      "800/800 [==============================] - 0s 607us/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 0.9937\n",
      "Epoch 403/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0239 - val_accuracy: 0.9875\n",
      "Epoch 404/500\n",
      "800/800 [==============================] - 0s 591us/step - loss: 0.0106 - accuracy: 0.9962 - val_loss: 0.0201 - val_accuracy: 0.9875\n",
      "Epoch 405/500\n",
      "800/800 [==============================] - 0s 598us/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 0.0237 - val_accuracy: 0.9875\n",
      "Epoch 406/500\n",
      "800/800 [==============================] - 0s 594us/step - loss: 0.0171 - accuracy: 0.9962 - val_loss: 0.0233 - val_accuracy: 0.9875\n",
      "Epoch 407/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0065 - accuracy: 0.9975 - val_loss: 0.0211 - val_accuracy: 0.9875\n",
      "Epoch 408/500\n",
      "800/800 [==============================] - 0s 592us/step - loss: 0.0104 - accuracy: 0.9975 - val_loss: 0.0211 - val_accuracy: 0.9937\n",
      "Epoch 409/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0103 - accuracy: 0.9975 - val_loss: 0.0322 - val_accuracy: 0.9812\n",
      "Epoch 410/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0134 - accuracy: 0.9962 - val_loss: 0.0314 - val_accuracy: 0.9812\n",
      "Epoch 411/500\n",
      "800/800 [==============================] - 0s 594us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 0.9937\n",
      "Epoch 412/500\n",
      "800/800 [==============================] - 0s 595us/step - loss: 0.0050 - accuracy: 0.9975 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
      "Epoch 413/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0141 - accuracy: 0.9987 - val_loss: 0.0228 - val_accuracy: 0.9875\n",
      "Epoch 414/500\n",
      "800/800 [==============================] - 0s 600us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 415/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0077 - accuracy: 0.9962 - val_loss: 0.0254 - val_accuracy: 0.9875\n",
      "Epoch 416/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0111 - accuracy: 0.9950 - val_loss: 0.0227 - val_accuracy: 0.9875\n",
      "Epoch 417/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0102 - accuracy: 0.9950 - val_loss: 0.0380 - val_accuracy: 0.9812\n",
      "Epoch 418/500\n",
      "800/800 [==============================] - 0s 583us/step - loss: 0.0129 - accuracy: 0.9950 - val_loss: 0.0352 - val_accuracy: 0.9812\n",
      "Epoch 419/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.0275 - val_accuracy: 0.9812\n",
      "Epoch 420/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0178 - accuracy: 0.9950 - val_loss: 0.0268 - val_accuracy: 0.9812\n",
      "Epoch 421/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.0269 - val_accuracy: 0.9875\n",
      "Epoch 422/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0065 - accuracy: 0.9962 - val_loss: 0.0338 - val_accuracy: 0.9875\n",
      "Epoch 423/500\n",
      "800/800 [==============================] - 0s 604us/step - loss: 0.0064 - accuracy: 0.9962 - val_loss: 0.0251 - val_accuracy: 0.9875\n",
      "Epoch 424/500\n",
      "800/800 [==============================] - 0s 611us/step - loss: 0.0032 - accuracy: 0.9987 - val_loss: 0.0261 - val_accuracy: 0.9875\n",
      "Epoch 425/500\n",
      "800/800 [==============================] - 0s 624us/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.0309 - val_accuracy: 0.9875\n",
      "Epoch 426/500\n",
      "800/800 [==============================] - 0s 599us/step - loss: 0.0065 - accuracy: 0.9987 - val_loss: 0.0323 - val_accuracy: 0.9875\n",
      "Epoch 427/500\n",
      "800/800 [==============================] - 0s 594us/step - loss: 0.0106 - accuracy: 0.9962 - val_loss: 0.0231 - val_accuracy: 0.9812\n",
      "Epoch 428/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0148 - accuracy: 0.9975 - val_loss: 0.0201 - val_accuracy: 0.9875\n",
      "Epoch 429/500\n",
      "800/800 [==============================] - 0s 623us/step - loss: 0.0080 - accuracy: 0.9975 - val_loss: 0.0191 - val_accuracy: 0.9937\n",
      "Epoch 430/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0148 - accuracy: 0.9962 - val_loss: 0.0210 - val_accuracy: 0.9937\n",
      "Epoch 431/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0111 - accuracy: 0.9962 - val_loss: 0.0172 - val_accuracy: 1.0000\n",
      "Epoch 432/500\n",
      "800/800 [==============================] - 0s 595us/step - loss: 0.0144 - accuracy: 0.9975 - val_loss: 0.0286 - val_accuracy: 0.9875\n",
      "Epoch 433/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0046 - accuracy: 0.9975 - val_loss: 0.0312 - val_accuracy: 0.9937\n",
      "Epoch 434/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.0276 - val_accuracy: 0.9875\n",
      "Epoch 435/500\n",
      "800/800 [==============================] - 0s 607us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 0.9875\n",
      "Epoch 436/500\n",
      "800/800 [==============================] - 1s 646us/step - loss: 0.0063 - accuracy: 0.9975 - val_loss: 0.0302 - val_accuracy: 0.9937\n",
      "Epoch 437/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0086 - accuracy: 0.9962 - val_loss: 0.0357 - val_accuracy: 0.9875\n",
      "Epoch 438/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0131 - accuracy: 0.9962 - val_loss: 0.0517 - val_accuracy: 0.9812\n",
      "Epoch 439/500\n",
      "800/800 [==============================] - 0s 611us/step - loss: 0.0072 - accuracy: 0.9975 - val_loss: 0.0462 - val_accuracy: 0.9875\n",
      "Epoch 440/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0053 - accuracy: 0.9975 - val_loss: 0.0408 - val_accuracy: 0.9750\n",
      "Epoch 441/500\n",
      "800/800 [==============================] - 0s 594us/step - loss: 0.0041 - accuracy: 0.9975 - val_loss: 0.0427 - val_accuracy: 0.9750\n",
      "Epoch 442/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0074 - accuracy: 0.9975 - val_loss: 0.0373 - val_accuracy: 0.9812\n",
      "Epoch 443/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0074 - accuracy: 0.9962 - val_loss: 0.0310 - val_accuracy: 0.9875\n",
      "Epoch 444/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0085 - accuracy: 0.9950 - val_loss: 0.0374 - val_accuracy: 0.9875\n",
      "Epoch 445/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0092 - accuracy: 0.9962 - val_loss: 0.0457 - val_accuracy: 0.9812\n",
      "Epoch 446/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0080 - accuracy: 0.9962 - val_loss: 0.0544 - val_accuracy: 0.9750\n",
      "Epoch 447/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0080 - accuracy: 0.9975 - val_loss: 0.0586 - val_accuracy: 0.9750\n",
      "Epoch 448/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0077 - accuracy: 0.9962 - val_loss: 0.0482 - val_accuracy: 0.9812\n",
      "Epoch 449/500\n",
      "800/800 [==============================] - 1s 651us/step - loss: 0.0099 - accuracy: 0.9962 - val_loss: 0.0424 - val_accuracy: 0.9875\n",
      "Epoch 450/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0399 - val_accuracy: 0.9875\n",
      "Epoch 451/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0202 - accuracy: 0.9937 - val_loss: 0.0364 - val_accuracy: 0.9937\n",
      "Epoch 452/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.0369 - val_accuracy: 0.9875\n",
      "Epoch 453/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0120 - accuracy: 0.9962 - val_loss: 0.0316 - val_accuracy: 0.9937\n",
      "Epoch 454/500\n",
      "800/800 [==============================] - 0s 595us/step - loss: 0.0065 - accuracy: 0.9962 - val_loss: 0.0318 - val_accuracy: 0.9875\n",
      "Epoch 455/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0076 - accuracy: 0.9987 - val_loss: 0.0260 - val_accuracy: 0.9875\n",
      "Epoch 456/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0122 - accuracy: 0.9975 - val_loss: 0.0253 - val_accuracy: 0.9875\n",
      "Epoch 457/500\n",
      "800/800 [==============================] - 0s 594us/step - loss: 0.0061 - accuracy: 0.9975 - val_loss: 0.0234 - val_accuracy: 0.9875\n",
      "Epoch 458/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0113 - accuracy: 0.9962 - val_loss: 0.0339 - val_accuracy: 0.9875\n",
      "Epoch 459/500\n",
      "800/800 [==============================] - 0s 595us/step - loss: 0.0073 - accuracy: 0.9962 - val_loss: 0.0510 - val_accuracy: 0.9875\n",
      "Epoch 460/500\n",
      "800/800 [==============================] - 0s 605us/step - loss: 0.0089 - accuracy: 0.9962 - val_loss: 0.0372 - val_accuracy: 0.9875\n",
      "Epoch 461/500\n",
      "800/800 [==============================] - 0s 624us/step - loss: 0.0059 - accuracy: 0.9975 - val_loss: 0.0318 - val_accuracy: 0.9812\n",
      "Epoch 462/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0092 - accuracy: 0.9975 - val_loss: 0.0307 - val_accuracy: 0.9812\n",
      "Epoch 463/500\n",
      "800/800 [==============================] - 0s 602us/step - loss: 0.0074 - accuracy: 0.9975 - val_loss: 0.0331 - val_accuracy: 0.9812\n",
      "Epoch 464/500\n",
      "800/800 [==============================] - 0s 587us/step - loss: 0.0055 - accuracy: 0.9975 - val_loss: 0.0378 - val_accuracy: 0.9812\n",
      "Epoch 465/500\n",
      "800/800 [==============================] - 0s 589us/step - loss: 0.0032 - accuracy: 0.9987 - val_loss: 0.0371 - val_accuracy: 0.9812\n",
      "Epoch 466/500\n",
      "800/800 [==============================] - 0s 605us/step - loss: 0.0129 - accuracy: 0.9937 - val_loss: 0.0464 - val_accuracy: 0.9812\n",
      "Epoch 467/500\n",
      "800/800 [==============================] - 0s 599us/step - loss: 0.0095 - accuracy: 0.9962 - val_loss: 0.0463 - val_accuracy: 0.9875\n",
      "Epoch 468/500\n",
      "800/800 [==============================] - 0s 618us/step - loss: 0.0155 - accuracy: 0.9950 - val_loss: 0.0500 - val_accuracy: 0.9812\n",
      "Epoch 469/500\n",
      "800/800 [==============================] - 0s 614us/step - loss: 0.0143 - accuracy: 0.9975 - val_loss: 0.0575 - val_accuracy: 0.9688\n",
      "Epoch 470/500\n",
      "800/800 [==============================] - 0s 595us/step - loss: 0.0079 - accuracy: 0.9962 - val_loss: 0.0535 - val_accuracy: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 471/500\n",
      "800/800 [==============================] - 1s 652us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9688\n",
      "Epoch 472/500\n",
      "800/800 [==============================] - 0s 577us/step - loss: 0.0079 - accuracy: 0.9987 - val_loss: 0.0457 - val_accuracy: 0.9688\n",
      "Epoch 473/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0057 - accuracy: 0.9987 - val_loss: 0.0395 - val_accuracy: 0.9750\n",
      "Epoch 474/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.0434 - val_accuracy: 0.9750\n",
      "Epoch 475/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0118 - accuracy: 0.9962 - val_loss: 0.0557 - val_accuracy: 0.9750\n",
      "Epoch 476/500\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0033 - accuracy: 0.9987 - val_loss: 0.0551 - val_accuracy: 0.9750\n",
      "Epoch 477/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.0479 - val_accuracy: 0.9750\n",
      "Epoch 478/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0102 - accuracy: 0.9962 - val_loss: 0.0438 - val_accuracy: 0.9875\n",
      "Epoch 479/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0460 - val_accuracy: 0.9812\n",
      "Epoch 480/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.0439 - val_accuracy: 0.9812\n",
      "Epoch 481/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0128 - accuracy: 0.9950 - val_loss: 0.0415 - val_accuracy: 0.9875\n",
      "Epoch 482/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0069 - accuracy: 0.9962 - val_loss: 0.0389 - val_accuracy: 0.9812\n",
      "Epoch 483/500\n",
      "800/800 [==============================] - 0s 578us/step - loss: 0.0032 - accuracy: 0.9975 - val_loss: 0.0403 - val_accuracy: 0.9750\n",
      "Epoch 484/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0111 - accuracy: 0.9975 - val_loss: 0.0450 - val_accuracy: 0.9750\n",
      "Epoch 485/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0062 - accuracy: 0.9962 - val_loss: 0.0481 - val_accuracy: 0.9812\n",
      "Epoch 486/500\n",
      "800/800 [==============================] - 0s 579us/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.0524 - val_accuracy: 0.9812\n",
      "Epoch 487/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0095 - accuracy: 0.9975 - val_loss: 0.0520 - val_accuracy: 0.9812\n",
      "Epoch 488/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0085 - accuracy: 0.9975 - val_loss: 0.0541 - val_accuracy: 0.9812\n",
      "Epoch 489/500\n",
      "800/800 [==============================] - 0s 580us/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 0.0588 - val_accuracy: 0.9812\n",
      "Epoch 490/500\n",
      "800/800 [==============================] - 0s 581us/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 0.0515 - val_accuracy: 0.9812\n",
      "Epoch 491/500\n",
      "800/800 [==============================] - 0s 588us/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 0.0529 - val_accuracy: 0.9812\n",
      "Epoch 492/500\n",
      "800/800 [==============================] - 0s 592us/step - loss: 0.0100 - accuracy: 0.9950 - val_loss: 0.0412 - val_accuracy: 0.9812\n",
      "Epoch 493/500\n",
      "800/800 [==============================] - 0s 582us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0415 - val_accuracy: 0.9812\n",
      "Epoch 494/500\n",
      "800/800 [==============================] - 0s 593us/step - loss: 0.0045 - accuracy: 0.9975 - val_loss: 0.0458 - val_accuracy: 0.9812\n",
      "Epoch 495/500\n",
      "800/800 [==============================] - 0s 585us/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0611 - val_accuracy: 0.9812\n",
      "Epoch 496/500\n",
      "800/800 [==============================] - 0s 594us/step - loss: 0.0104 - accuracy: 0.9962 - val_loss: 0.0498 - val_accuracy: 0.9750\n",
      "Epoch 497/500\n",
      "800/800 [==============================] - 0s 590us/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0430 - val_accuracy: 0.9750\n",
      "Epoch 498/500\n",
      "800/800 [==============================] - 0s 584us/step - loss: 0.0153 - accuracy: 0.9975 - val_loss: 0.0463 - val_accuracy: 0.9750\n",
      "Epoch 499/500\n",
      "800/800 [==============================] - 0s 586us/step - loss: 0.0138 - accuracy: 0.9937 - val_loss: 0.0515 - val_accuracy: 0.9750\n",
      "Epoch 500/500\n",
      "800/800 [==============================] - 1s 796us/step - loss: 0.0032 - accuracy: 0.9987 - val_loss: 0.0497 - val_accuracy: 0.9750\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from numpy import zeros, newaxis\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from joblib import Parallel,delayed\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras import utils as np_utils                     \n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from keras.layers import Dense,Flatten,Conv1D,MaxPooling1D,Dropout,BatchNormalization,PReLU\n",
    "from tensorflow.keras import datasets,layers,models\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy import unique\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras import backend as K \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from sklearn.utils import shuffle\n",
    "from numpy.random import RandomState\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def baseline_als(y, lam, p, niter=10):\n",
    "    L = len(y)\n",
    "    D = sparse.diags([1, -2, 1], [0, -1, -2], shape=(L, L - 2))\n",
    "    w = np.ones(L)\n",
    "    for i in range(niter):\n",
    "        W = sparse.spdiags(w, 0, L, L)\n",
    "        Z = W + lam * D.dot(D.transpose())\n",
    "        z = spsolve(Z, w * y)\n",
    "        w = p * (y > z) + (1 - p) * (y < z)\n",
    "    return z\n",
    "#CNNBinaryFinalmodel7\n",
    "# load in CNN\n",
    "weightsfile ='CNNBinaryFinalmodel7.h5'\n",
    "modelfile = 'CNNBinaryFinalmodel7.json'\n",
    "#CNNRegression_TransferWasteWater\n",
    "# load model from json\n",
    "json_file = open(modelfile, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "base_model = keras.models.model_from_json(loaded_model_json)\n",
    "base_model.load_weights(weightsfile)\n",
    "print(base_model.summary())\n",
    "# remove the last 2 dense FC layers and freeze it\n",
    "base_model.pop()\n",
    "base_model.pop()\n",
    "base_model.trainable = False\n",
    "\n",
    "#new_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "inp = base_model.input\n",
    "\n",
    "last = base_model.layers[-1].output\n",
    "#x = Flatten()(last)\n",
    "#x = Dense(1000, activation='relu', name='fc1')(x)\n",
    "#x = Dropout(0.3)(x)\n",
    "x = Dense(1, activation='sigmoid', name='predictions')(last)\n",
    "\n",
    "model2 = Model(inp, x)\n",
    "\n",
    "#base_model.layers[0].trainable = False\n",
    "#base_model.layers[1].trainable = False\n",
    "#base_model.layers[2].trainable = False\n",
    "#model2.layers[0].trainable = True\n",
    "model2.layers[1].trainable = True\n",
    "model2.layers[2].trainable = True\n",
    "model2.layers[3].trainable = True\n",
    "model2.layers[4].trainable = True\n",
    "model2.layers[5].trainable = True\n",
    "model2.layers[6].trainable = True\n",
    "model2.layers[7].trainable = False\n",
    "model2.layers[8].trainable = False\n",
    "model2.layers[9].trainable = False\n",
    "model2.layers[10].trainable = False\n",
    "model2.layers[11].trainable = False\n",
    "model2.layers[12].trainable = False\n",
    "model2.layers[13].trainable = False\n",
    "model2.layers[14].trainable = False\n",
    "#model2.layers[15].trainable = True\n",
    "\n",
    "print(model2.summary())\n",
    "\n",
    "fnames=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/20220401_Wastewater_400/*'))\n",
    "files=[np.loadtxt(f) for f in fnames]\n",
    "map=[f[:,3] for f in files]\n",
    "map=[np.reshape(m,(-1,1011)) for m in map]\n",
    "map_full = np.concatenate([m[:,:] for m in map],axis=0)\n",
    "\n",
    "y = np.concatenate([np.repeat(i, len(m)) for i, m in enumerate(map)], axis=0)\n",
    "map_full= savgol_filter(map_full, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map_full)\n",
    "map_full= np.subtract(map_full, back)\n",
    "map_full= np.reshape(map_full, (-1, 1011))\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map_full[:,999:1000]\n",
    "X=map_full/divide\n",
    "\n",
    "x_train, x_test, y_train, y_test=train_test_split(X,y,test_size=0.5,stratify=y,shuffle=True)\n",
    "x_val, x_test, y_val, y_test=train_test_split(x_test,y_test,test_size=0.8,stratify=y_test,shuffle=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "\n",
    "pca = PCA(n_components=22)\n",
    "pca.fit(x_train)\n",
    "x_train= pca.transform(x_train)\n",
    "x_val= pca.transform(x_val)\n",
    "x_test= pca.transform(x_test)\n",
    "\n",
    "x_train_cnn= np.reshape(x_train, (x_train.shape[0], x_train.shape[1],1))\n",
    "x_val_cnn= np.reshape(x_val, (x_val.shape[0], x_val.shape[1],1))\n",
    "x_test_cnn= np.reshape(x_test, (x_test.shape[0], x_test.shape[1],1))\n",
    "\n",
    "base_learning_rate = 0.001\n",
    "#sparse_categorical\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "finetune_history=model2.fit(x_train_cnn, y_train,epochs=500,validation_data=(x_val_cnn,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e17ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on 4 trained class\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i,  \"{:0.2f}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",fontsize=20,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "#bars = ('C', '0.1','1','100','1000')\n",
    "#x_pos = np.arange(len(bars))\n",
    "\n",
    "#y_pred1 = model2.predict_classes(x_test_cnn)\n",
    "#y_predic=np.argmax(y_pred1,axis=1)\n",
    "\n",
    "#binary\n",
    "#X_cleaned= pca.transform(X_cleaned)\n",
    "#X_cleaned_cnn= np.reshape(X_cleaned, (X_cleaned.shape[0], X_cleaned.shape[1],1))\n",
    "#y_pred1 = (model2.predict(x_test_cnn) > 0.5).astype(\"int32\")\n",
    "\n",
    "#multiclass!!\n",
    "y_pred1=model2.predict(x_test_cnn) \n",
    "y_pred1=np.argmax(y_pred1, axis=1)\n",
    "\n",
    "cn_matrix = confusion_matrix(y_test,y_pred1)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "class_names=['0.1 \\u03bcg/L','1 \\u03bcg/L','100 \\u03bcg/L','1000 \\u03bcg/L']\n",
    "#plt.figure(figsize=(5,5))\n",
    "plot_confusion_matrix(cn_matrix, classes=class_names, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba002b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1011)\n",
      "(400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_iforest.py:453: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_iforest.py:453: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_iforest.py:314: RuntimeWarning: invalid value encountered in less\n",
      "  is_inlier[self.decision_function(X) < 0] = -1\n"
     ]
    }
   ],
   "source": [
    "fname5=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/20220401_Wastewater_control/*'))\n",
    "file5=[np.loadtxt(f) for f in fname5]\n",
    "map5=[f[:,3] for f in file5]\n",
    "map5=[np.reshape(m,(-1,1011)) for m in map5]\n",
    "map_full5= np.concatenate([m[:,:] for m in map5],axis=0)\n",
    "map_full5=np.array(map_full5)\n",
    "Y_tes=np.repeat(0,400)\n",
    "map_full5= savgol_filter(map_full5, 11, 3, axis=1)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map_full5)\n",
    "map_full5= np.subtract(map_full5, back)\n",
    "map_full5= np.reshape(map_full5, (-1, 1011))\n",
    "\n",
    "number = map_full5.shape[0]\n",
    "random = np.random.choice(number, size=400, replace=False)\n",
    "map_full5=map_full5[random,:]\n",
    "\n",
    "divide5=map_full5[:,999:1000]\n",
    "X_tes=map_full5/divide5\n",
    "\n",
    "clf = IsolationForest(max_samples=1, random_state=4, contamination=0.01)\n",
    "#identify outliers:\n",
    "y_pred = clf.fit_predict(X_tes)\n",
    "#Remove outliers where 1 represent inliers and -1 represent outliers:\n",
    "X_cleaned, y_cleaned =X_tes[(y_pred != -1), :], Y_tes[(y_pred != -1)]\n",
    "\n",
    "\n",
    "print(X_cleaned.shape)\n",
    "print(y_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb59eb7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(hist['epoch'], hist['accuracy'],\n",
    "           label='Train Accuracy')\n",
    "    plt.plot(hist['epoch'], hist['val_accuracy'],\n",
    "           label = 'Val Accuracy')\n",
    "    plt.ylim([0,1.1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(finetune_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d398ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.917 0.083]\n",
      " [  nan   nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i,  \"{:0.2f}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",fontsize=20,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "#bars = ('C', '0.1','1','100','1000')\n",
    "#x_pos = np.arange(len(bars))\n",
    "\n",
    "#y_pred1 = model2.predict_classes(x_test_cnn)\n",
    "#y_predic=np.argmax(y_pred1,axis=1)\n",
    "\n",
    "#binary\n",
    "X_cleaned= pca.transform(X_cleaned)\n",
    "X_cleaned_cnn= np.reshape(X_cleaned, (X_cleaned.shape[0], X_cleaned.shape[1],1))\n",
    "y_pred1 = (model2.predict(X_cleaned_cnn) > 0.5).astype(\"int32\")\n",
    "\n",
    "#multiclass!!\n",
    "#y_pred1=model2.predict(X_cleaned_cnn) \n",
    "#y_pred1=np.argmax(y_pred1, axis=1)\n",
    "\n",
    "cn_matrix = confusion_matrix(y_cleaned,y_pred1)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "class_names=['below','above']\n",
    "#plt.figure(figsize=(5,5))\n",
    "plot_confusion_matrix(cn_matrix, classes=class_names, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot out embedding \n",
    "\n",
    "tsne = TSNE(3, verbose=1)\n",
    "tsne_proj = tsne.fit_transform(test_embeddings)\n",
    "cmap = cm.get_cmap('tab20')\n",
    "num_categories = 1\n",
    "for lab in range(num_categories):\n",
    "    indices = test_predictions == lab\n",
    "    ax.scatter(tsne_proj[indices, 0],\n",
    "               tsne_proj[indices, 1],\n",
    "               tsne_proj[indices, 2],\n",
    "               c=np.array(cmap(lab)).reshape(1, 4),\n",
    "               label=lab,\n",
    "               alpha=0.5)\n",
    "ax.legend(fontsize='large', markerscale=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tunning \n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "model2.layers[0].trainable = True\n",
    "model2.layers[1].trainable = True\n",
    "model2.layers[2].trainable = True\n",
    "model2.layers[3].trainable = True\n",
    "model2.layers[4].trainable = True\n",
    "model2.layers[5].trainable = True\n",
    "model2.layers[6].trainable = True\n",
    "model2.layers[7].trainable = True\n",
    "model2.layers[8].trainable = True\n",
    "model2.layers[9].trainable = True\n",
    "model2.layers[10].trainable = True\n",
    "model2.layers[11].trainable = True\n",
    "model2.layers[12].trainable = True\n",
    "model2.layers[13].trainable = True\n",
    "model2.layers[14].trainable = True\n",
    "model2.summary()\n",
    "\n",
    "# It's important to recompile your model after you make any changes\n",
    "# to the `trainable` attribute of any inner layer, so that your changes\n",
    "# are take into account\n",
    "model2.compile(optimizer=keras.optimizers.Adam(1e-5),  \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train end-to-end. Be careful to stop before you overfit!\n",
    "finetune_history=model2.fit(x_train_cnn, y_train, epochs=100,validation_data=(x_val_cnn,y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(hist['epoch'], hist['accuracy'],\n",
    "           label='Train Accuracy')\n",
    "    plt.plot(hist['epoch'], hist['val_accuracy'],\n",
    "           label = 'Val Accuracy')\n",
    "    plt.ylim([0,1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(finetune_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e674e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from numpy import zeros, newaxis\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from joblib import Parallel,delayed\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras import utils as np_utils                     \n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout,BatchNormalization,PReLU\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy import unique\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras import backend as K \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from keras.models import Model\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    #plt.title(title)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=25,**hfont)\n",
    "    plt.yticks(tick_marks, classes, fontsize=25,**hfont)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i,  \"{:0.1f}\".format(cm[i, j]),fontsize=15,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",**hfont)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.ylabel(r'Predicted label ($0.5\\times10^X$pM)',fontsize=25,**hfont)\n",
    "    plt.xlabel(r'True label ($0.5\\times10^X$pM)',fontsize=25,**hfont)\n",
    "    #plt.xlabel(r'Predicted label ($0.5\\times10^X$pM)',fontsize=20,**hfont)\n",
    "hfont = {'fontname':'Helvetica'}\n",
    "\n",
    "cn_matrix=np.array([[99.6,0.0,9.4,0.3,1.8,2.4,0.0,0.0],\n",
    "                    [0.1,93.9,3.1,0.0,0.0,0.0,0.0,0.0],\n",
    "                    [0.2,6.1,87.4,0.0,0.0,0.0,0.0,0.0],\n",
    "                    [0.0,0.0,0.0,99.1,0.6,0.0,0.0,0.0],\n",
    "                    [0.1,0.0,0.0,0.6,97.5,0.0,0.0,0.0],\n",
    "                    [0.0,0.0,0.0,0.0,0.0,97.6,0.0,0.0],\n",
    "                    [0.0,0.0,0.0,0.0,0.0,0.0,100.0,0.0],\n",
    "                    [0.0,0.0,0.0,0.0,0.0,0.0,0.0,100.0]])\n",
    "\n",
    "\n",
    "\n",
    "class_names = ['C','-2','-1','0','1','2','3','4']\n",
    "#(\\u03bcg/L)\n",
    "#class_names=['I','II','III','IV']\n",
    "#plt.figure(figsize=(5, 5))\n",
    "#plt.title(r'$5\\times10^2$ CFU/mL', fontsize=20,**hfont)\n",
    "#10$^{-2}$\n",
    "#class_names = [r'Cr$^{6+}$',r'As$^{3+}$']\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.tick_params(direction='in', length=0, width=1, colors='black')\n",
    "plot_confusion_matrix(cn_matrix, classes=class_names, normalize=False)\n",
    "plt.savefig('HM_pnas_as_cm_main.png', dpi=1200,bbox_inches='tight')\n",
    "#bbox_inches='tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0764539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cr\n",
    "cn_matrix=np.array([[99.7,0.2,0.0,0.0,0.0,0.0,0.1,0.1],\n",
    "                    [13.9,85.8, 0.0,0.4,0.0,0.0,0.0,0.0],\n",
    "                    [0.0,0.0,100.0,0.0,0.0,0.0,0.0,0.0],\n",
    "                    [1.2,0.8,0.0,98.0,0.0,0.0,0.0,0.0],\n",
    "                    [2.0,0.0,0.0,0.0,97.2,0.4,0.4,0.0],\n",
    "                    [0.0,0.0,0.0,0.0,0.4,99.6,0.0,0.0],\n",
    "                    [0.0,0.0,0.0,0.0,0.0,0.8,99.2,0.0],\n",
    "                    [0.8,0.0,0.0,0.0,0.0,0.0,0.0,99.2]])\n",
    "cn_matrix=np.array([[98.1,1.3,0.0,0.1,0.1,0.0,0.0,0.3],\n",
    "                    [1.1,97.5,0.0,1.4,0.0,0.0,0.0,0.0],\n",
    "                    [0.0,0.1,99.9,0.0,0.0,0.0,0.0,0.0],\n",
    "                    [0.2,0.7,0.1,99.0,0.0,0.0,0.0,0.0],\n",
    "                    [0.0,0.0,0.0,0.0,99.4,0.6,0.0,0.0],\n",
    "                    [0.0,0.0,0.0,0.0,0.5,98.5,0.9,0.0],\n",
    "                    [0.0,0.0,0.0,0.0,0.6,0.6,98.6,0.1],\n",
    "                    [0.8,0.0,0.0,0.0,0.0,0.0,0.0,99.2]])\n",
    "\n",
    "cn_matrix=np.array([[99.8,0.1,0.1,0.1,0.0,0.0,0.1,0.1],\n",
    "                    [1.2,94.6,4.2,0.0,0.0,0.0,0.0,0.0],\n",
    "                    [9.4,4.2,86.4,0.0,0.0,0.0,0.0,0.0],\n",
    "                    [0.4,0.0,0.0,99.6,0.0,0.0,0.0,0.0],\n",
    "                    [0.0,0.0,0.0,0.0,100.0,0.0,0.0,0.0],\n",
    "                    [0.8,0.0,0.0,0.0,0.0,99.2,0.0,0.0],\n",
    "                    [0.0,0.0,0.0,0.0,0.0,0.0,100.0,0.0],\n",
    "                    [0.4,0.0,0.0,0.0,0.0,0.0,0.0,99.6]])\n",
    "cn_matrix=np.array([[98.3,0.1,1.2,0.1,0.0,0.2,0.0,0.0],\n",
    "                    [0.4,96.4,3.2,0.0,0.0,0.0,0.0,0.0],\n",
    "                    [0.7,3.4,95.8,0.0,0.0,0.0,0.0,0.0],\n",
    "                    [0.5,0.0,0.1,99.4,0.0,0.0,0.0,0.0],\n",
    "                    [0.0,0.0,0.0,0.2,99.8,0.0,0.0,0.0],\n",
    "                    [0.0,0.0,0.0,0.0,0.0,99.9,0.0,0.1],\n",
    "                    [0.1,0.0,0.0,0.0,0.6,0.6,99.8,0.1],\n",
    "                    [0.0,0.0,0.0,0.0,0.0,0.1,0.0,99.9]])\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "#Fit the model\n",
    "\n",
    "cf_matrix=np.array([[0.988,0.012],\n",
    "                    [0.01,0.99]])\n",
    "sns.heatmap(cf_matrix, annot=True, \n",
    "            fmt='.2%', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256eaddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(python_version())\n",
    "fname1=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/test_0.1_map2/*'))\n",
    "file1=[np.loadtxt(f) for f in fname1]\n",
    "map1=[f[:,3] for f in file1]\n",
    "map1=np.reshape(map1,(-1,1011))\n",
    "map1= savgol_filter(map1, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map1)\n",
    "map1= np.subtract(map1, back)\n",
    "map1= np.reshape(map1, (-1, 1011))\n",
    "\n",
    "number1 = map1.shape[0]\n",
    "random1 = np.random.choice(number1, size=10, replace=False)\n",
    "map1=map1[random1,:]\n",
    "y_1=np.repeat(0,10)\n",
    "\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map1[:,999:1000]\n",
    "X_1=map1/divide\n",
    "\n",
    "print(X_1.shape)\n",
    "print(y_1)\n",
    "\n",
    "fname2=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/test_0.1_map3/*'))\n",
    "file2=[np.loadtxt(f) for f in fname2]\n",
    "map2=[f[:,3] for f in file2]\n",
    "map2=np.reshape(map2,(-1,1011))\n",
    "\n",
    "map2= savgol_filter(map2, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map2)\n",
    "map2= np.subtract(map2, back)\n",
    "map2= np.reshape(map2, (-1, 1011))\n",
    "\n",
    "number2 = map2.shape[0]\n",
    "random2 = np.random.choice(number2, size=10, replace=False)\n",
    "map2=map2[random2,:]\n",
    "y_2=np.repeat(0,10)\n",
    "\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map2[:,999:1000]\n",
    "X_2=map2/divide\n",
    "\n",
    "print(X_2.shape)\n",
    "print(y_2)\n",
    "\n",
    "fname3=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/test_1_map1/*'))\n",
    "file3=[np.loadtxt(f) for f in fname3]\n",
    "map3=[f[:,3] for f in file3]\n",
    "map3=np.reshape(map3,(-1,1011))\n",
    "\n",
    "map3= savgol_filter(map3, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map3)\n",
    "map3= np.subtract(map3, back)\n",
    "map3= np.reshape(map3, (-1, 1011))\n",
    "\n",
    "number3 = map3.shape[0]\n",
    "random3 = np.random.choice(number3, size=10, replace=False)\n",
    "map3=map3[random3,:]\n",
    "y_3=np.repeat(0,10)\n",
    "\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map3[:,999:1000]\n",
    "X_3=map3/divide\n",
    "\n",
    "print(X_3.shape)\n",
    "print(y_3)\n",
    "\n",
    "fname4=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/test_1_map2/*'))\n",
    "file4=[np.loadtxt(f) for f in fname4]\n",
    "map4=[f[:,3] for f in file4]\n",
    "map4=np.reshape(map4,(-1,1011))\n",
    "\n",
    "map4= savgol_filter(map4, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map4)\n",
    "map4= np.subtract(map4, back)\n",
    "map4= np.reshape(map4, (-1, 1011))\n",
    "\n",
    "number4 = map4.shape[0]\n",
    "random4 = np.random.choice(number4, size=10, replace=False)\n",
    "map4=map4[random4,:]\n",
    "y_4=np.repeat(0,10)\n",
    "\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map4[:,999:1000]\n",
    "X_4=map4/divide\n",
    "\n",
    "print(X_4.shape)\n",
    "print(y_4)\n",
    "\n",
    "fname5=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/test_1_map3/*'))\n",
    "file5=[np.loadtxt(f) for f in fname5]\n",
    "map5=[f[:,3] for f in file5]\n",
    "map5=np.reshape(map5,(-1,1011))\n",
    "\n",
    "map5= savgol_filter(map5, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map5)\n",
    "map5= np.subtract(map5, back)\n",
    "map5= np.reshape(map5, (-1, 1011))\n",
    "\n",
    "number5 = map5.shape[0]\n",
    "random5 = np.random.choice(number5, size=10, replace=False)\n",
    "map5=map5[random5,:]\n",
    "y_5=np.repeat(0,10)\n",
    "\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map5[:,999:1000]\n",
    "X_5=map5/divide\n",
    "\n",
    "print(X_5.shape)\n",
    "print(y_5)\n",
    "\n",
    "fname6=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/test_100_map1/*'))\n",
    "file6=[np.loadtxt(f) for f in fname6]\n",
    "map6=[f[:,3] for f in file6]\n",
    "map6=np.reshape(map6,(-1,1011))\n",
    "\n",
    "map6= savgol_filter(map6, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map6)\n",
    "map6= np.subtract(map6, back)\n",
    "map6= np.reshape(map6, (-1, 1011))\n",
    "\n",
    "number6 = map6.shape[0]\n",
    "random6 = np.random.choice(number6, size=10, replace=False)\n",
    "map6=map6[random6,:]\n",
    "y_6=np.repeat(1,10)\n",
    "\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map6[:,999:1000]\n",
    "X_6=map6/divide\n",
    "\n",
    "print(X_6.shape)\n",
    "print(y_6)\n",
    "\n",
    "fname7=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/test_100_map2/*'))\n",
    "file7=[np.loadtxt(f) for f in fname7]\n",
    "map7=[f[:,3] for f in file7]\n",
    "map7=np.reshape(map7,(-1,1011))\n",
    "\n",
    "map7= savgol_filter(map7, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map7)\n",
    "map7= np.subtract(map7, back)\n",
    "map7= np.reshape(map7, (-1, 1011))\n",
    "\n",
    "number7 = map7.shape[0]\n",
    "random7 = np.random.choice(number7, size=10, replace=False)\n",
    "map7=map7[random7,:]\n",
    "y_7=np.repeat(1,10)\n",
    "\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map7[:,999:1000]\n",
    "X_7=map7/divide\n",
    "\n",
    "print(X_7.shape)\n",
    "print(y_7)\n",
    "\n",
    "fname8=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/test_100_map3/*'))\n",
    "file8=[np.loadtxt(f) for f in fname8]\n",
    "map8=[f[:,3] for f in file8]\n",
    "map8=np.reshape(map8,(-1,1011))\n",
    "\n",
    "map8= savgol_filter(map8, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map8)\n",
    "map8= np.subtract(map8, back)\n",
    "map8= np.reshape(map8, (-1, 1011))\n",
    "\n",
    "number8 = map8.shape[0]\n",
    "random8 = np.random.choice(number8, size=10, replace=False)\n",
    "map8=map8[random8,:]\n",
    "y_8=np.repeat(1,10)\n",
    "\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map8[:,999:1000]\n",
    "X_8=map8/divide\n",
    "\n",
    "print(X_8.shape)\n",
    "print(y_8)\n",
    "\n",
    "fname9=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/test_1000_map1/*'))\n",
    "file9=[np.loadtxt(f) for f in fname9]\n",
    "map9=[f[:,3] for f in file9]\n",
    "map9=np.reshape(map9,(-1,1011))\n",
    "\n",
    "map9= savgol_filter(map9, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map9)\n",
    "map9= np.subtract(map9, back)\n",
    "map9= np.reshape(map9, (-1, 1011))\n",
    "\n",
    "number9 = map9.shape[0]\n",
    "random9 = np.random.choice(number9, size=10, replace=False)\n",
    "map9=map9[random9,:]\n",
    "y_9=np.repeat(1,10)\n",
    "\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map9[:,999:1000]\n",
    "X_9=map9/divide\n",
    "\n",
    "print(X_9.shape)\n",
    "print(y_9)\n",
    "\n",
    "\n",
    "fname10=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/test_1000_map2/*'))\n",
    "file10=[np.loadtxt(f) for f in fname10]\n",
    "map10=[f[:,3] for f in file10]\n",
    "map10=np.reshape(map10,(-1,1011))\n",
    "\n",
    "map10= savgol_filter(map10, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map10)\n",
    "map10= np.subtract(map10, back)\n",
    "map10= np.reshape(map10, (-1, 1011))\n",
    "\n",
    "number10 = map10.shape[0]\n",
    "random10 = np.random.choice(number10, size=10, replace=False)\n",
    "map10=map10[random10,:]\n",
    "y_10=np.repeat(1,10)\n",
    "\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map10[:,999:1000]\n",
    "X_10=map10/divide\n",
    "\n",
    "print(X_10.shape)\n",
    "print(y_10)\n",
    "\n",
    "fname11=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/test_1000_map3/*'))\n",
    "file11=[np.loadtxt(f) for f in fname11]\n",
    "map11=[f[:,3] for f in file11]\n",
    "map11=np.reshape(map11,(-1,1011))\n",
    "\n",
    "map11= savgol_filter(map11, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map11)\n",
    "map11= np.subtract(map11, back)\n",
    "map11= np.reshape(map11, (-1, 1011))\n",
    "\n",
    "number11 = map11.shape[0]\n",
    "random11 = np.random.choice(number11, size=10, replace=False)\n",
    "map11=map11[random11,:]\n",
    "y_11=np.repeat(1,10)\n",
    "\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map11[:,999:1000]\n",
    "X_11=map11/divide\n",
    "\n",
    "print(X_11.shape)\n",
    "print(y_11)\n",
    "\n",
    "\n",
    "X=np.concatenate([X_0,X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8,X_9,X_10,X_11], axis=0)\n",
    "y=np.concatenate([y_0,y_1,y_2,y_3,y_4,y_5,y_6,y_7,y_8,y_9,y_10,y_11], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file\n",
    "#model.save(\"CNNmodel.h5\")\n",
    "#print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "#For CNN\n",
    "import math\n",
    "\n",
    "print(y_preds_1d)\n",
    "\n",
    "y_exponent=10**y_preds_1d\n",
    "y_concentration=y_exponent*0.5\n",
    "print(y_concentration)\n",
    "\n",
    "print(np.median(y_concentration[0:240]))\n",
    "print(np.mean(y_concentration[0:240]))\n",
    "print(np.std(y_concentration[0:240]))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(y_preds_1d)\n",
    "df.to_csv('PNAS_Cr_CNN_finalwith2nd.csv',index=False)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "fnames=sorted(glob('/Users/hongwei/Desktop/MachineLearningdata/031621_Ar_replicate_0.5pM/*'))\n",
    "files=[np.loadtxt(f) for f in fnames]\n",
    "map=[f[:,3] for f in files]\n",
    "map=[np.reshape(m,(-1,1011)) for m in map]\n",
    "map_full = np.concatenate([m[:,:] for m in map],axis=0)\n",
    "\n",
    "y = np.concatenate([np.repeat(i, len(m)) for i, m in enumerate(map)], axis=0)\n",
    "map_full= savgol_filter(map_full, 11, 3, axis=0)\n",
    "back = Parallel(n_jobs=8)(delayed(baseline_als)(j, 100000, 0.001) for j in map_full)\n",
    "map_full= np.subtract(map_full, back)\n",
    "map_full= np.reshape(map_full, (-1, 1011))\n",
    "#X= minmax_scale(map_full, axis=1)\n",
    "divide=map_full[:,999:1000]\n",
    "X=map_full/divide\n",
    "\n",
    "\n",
    "pca.fit(X)\n",
    "\n",
    "X_1= pca.transform(X)\n",
    "X_1 = np.reshape(X_1, (X_1.shape[0], X_1.shape[1],1))\n",
    "\n",
    "y_pred = model.predict(X_1)\n",
    "\n",
    "print(np.median(y_pred))\n",
    "print(np.std(y_pred))\n",
    "\n",
    "\n",
    "#Plot out figures\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "\n",
    "y_preds_1d=y_predict.flatten()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bp=sns.boxplot(x=y_rus,y=y_preds_1d,linewidth=1.3,flierprops = dict(markerfacecolor = 'dodgerblue',markeredgecolor='dodgerblue', markersize = 3,marker='o',),whis=[0.5,99.5])\n",
    "\n",
    "hfont = {'fontname':'Helvetica'}\n",
    "\n",
    "labels = [\"0\",\"5 fM\",\"5 pM\",\"5 nM\",\"5 \\u03bcM\",\"5 mM\"]\n",
    "label = [\" \",\"5 fM\",\"5 pM\",\"5 nM\",\"5 \\u03bcM\",\"5 mM\"]\n",
    "ax.tick_params(direction='in', length=8, width=3, colors='black')\n",
    "\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator(1))\n",
    "\n",
    "ax.tick_params(direction='in',which='both', width=1)\n",
    "ax.tick_params(direction='in',which='minor', length=4)\n",
    "\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator(1))\n",
    "\n",
    "ax.tick_params(which='both', width=1)\n",
    "ax.tick_params(which='minor', length=4)\n",
    "\n",
    "ax.set_xticks([0,1,4,7,10,13])\n",
    "ax.set_yticks([0,1,4,7,10,13])\n",
    "ax.set_xticklabels(label,fontsize=18,**hfont)\n",
    "ax.set_yticklabels(labels,fontsize=18,**hfont)\n",
    "ax.set_xlabel('True Concentrations',fontsize=20,**hfont)\n",
    "ax.set_ylabel('Predicted Concentrations',fontsize=20,**hfont)\n",
    "\n",
    "x = np.linspace(0, 14, 13)\n",
    "plt.plot(x, x + 0, 'black')  # solid green\n",
    "plt.xlim(-0.5,13.5)\n",
    "plt.ylim(-0.1,13.5,1)\n",
    "plt.axhspan(-0.1, 1.8, facecolor='0.9')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
